{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b7476a",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4 Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89d459",
   "metadata": {},
   "source": [
    "This homework is an extension of homework 4, where you will be implementing the Transformer architecture. For this assignment, all the things you need to implement is in the file `python/needle/nn/nn_transformer.py`. Other things in the needle library remains the same. This homework extension is built on homework 4, so make sure to copy the solutions from homework 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set up the assignment —— run only on Drive!\n",
    "from google.colab import drive, userdata\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p 10714\n",
    "%cd /content/drive/MyDrive/10714\n",
    "\n",
    "token = userdata.get(\"GITHUB_TOKEN\")\n",
    "\n",
    "# On our first run only: clone our remote repo where we're pushing changes\n",
    "!rm -r hw4x/\n",
    "!git clone https://{token}@github.com/jchanke/dlsys-hw4.git hw4x/\n",
    "\n",
    "# Enter the hw4 directory\n",
    "%cd /content/drive/MyDrive/10714/hw4x\n",
    "\n",
    "# On subsequent runs only: pull changes we've pushed from our local machine\n",
    "# !git pull https://{token}@github.com/jchanke/dlsys-hw4.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1d9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED FOR MUGRADE\n",
    "MY_API_KEY = \"yGDY6yxkoXscTEBJyA4O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c9fb467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "  CMake.\n",
      "\n",
      "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "  to tell CMake that the project requires at least <min> but has been updated\n",
      "  to work with policies introduced by <max> or earlier.\n",
      "\n",
      "\u001b[0m\n",
      "-- Found pybind11: /home/joey/10-714/.venv/lib/python3.13/site-packages/pybind11/include (found version \"3.0.1\")\n",
      "\u001b[0mCUDA_TOOLKIT_ROOT_DIR not found or specified\u001b[0m\n",
      "-- Could NOT find CUDA (missing: CUDA_TOOLKIT_ROOT_DIR CUDA_NVCC_EXECUTABLE CUDA_INCLUDE_DIRS CUDA_CUDART_LIBRARY) \n",
      "-- Configuring done (0.2s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /home/joey/10-714/hw4/build\n",
      "make[1]: Entering directory '/home/joey/10-714/hw4/build'\n",
      "[100%] Built target ndarray_backend_cpu\n",
      "make[1]: Leaving directory '/home/joey/10-714/hw4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45349235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=./python\n",
      "env: NEEDLE_BACKEND=nd\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54d7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5945207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PTB dataset\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea5c0a",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2f639",
   "metadata": {},
   "source": [
    "In the previous homework you have implemented two sequence models, the Recurrent Neural Network, and Long Short-Term Memory. These models were once the state-of-the-art and default architecture choices on sequence modelling tasks, including language generation, until recently when the famous paper \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" (Vaswani et al. 2017) came out in 2017. Since then, Transformers, a model architecture introduced in the aforementioned paper, have become the standard and most performant class of model on language tasks. \n",
    "\n",
    "You will be implementing a Transformer in `python/needle/nn/nn_transformer.py`.\n",
    "\n",
    "Transformers are composed of three mains components that you will implement. \n",
    "1. A masked multi-head attention mechanism that adaptively focuses on different timesteps of a sequence. \n",
    "2. A residual block consisting of the attention layer followed by a two-layer neural network applied independently at each timestep. \n",
    "3. A Transformer model consisting of several stacked residual blocks (in this homework you will implement a decoder-only transformer).\n",
    "\n",
    "![model](https://miro.medium.com/v2/1*ZCFSvkKtppgew3cc7BIaug.png)\n",
    "\n",
    "The above is a photo of the Transformer architecture from Vaswani et al. 2017. The version of the transformer you will implement is nearly identical, but has layer normalization applied at the start of each residual block (referred to as a [prenorm variant](https://arxiv.org/abs/2002.04745) of the Transformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f094ff30",
   "metadata": {},
   "source": [
    "## Part 1: Implementing the Multi-Head Attention Activation Layer\n",
    "\n",
    "In this subproblem, you will be implementing the `forward` function of a \"base\"\n",
    "attention activation layer `MultiHeadAttention` in\n",
    "`python/needle/nn/nn_transformer.py`. This activation layer will take in three\n",
    "inputs: \n",
    "\n",
    " - multi-head queries $Q \\in R^\\mathcal{B \\times H \\times T \\times D}$\n",
    " - keys $K \\in R^\\mathcal{B \\times H \\times T \\times D}$, and \n",
    " - values $V \\in R^\\mathcal{B \\times H \\times T \\times D}$ \n",
    "\n",
    "where $B$ is the batch size, $H$ is the number of attention heads, $T$ is the\n",
    "sequence length, and $D$ is the hidden dimension. \n",
    "\n",
    "The attention output $X \\in R^{B \\times H \\times T \\times D}$ is computed as\n",
    "follows: \n",
    "\n",
    "  $X = \\text{softmax}(\\frac{Q K^T}{\\sqrt{D}}) V$\n",
    "\n",
    "Note that the matrix multiplications above are batched. This functionality is\n",
    "not natively supported in needle yet, so we have provided a convenient function\n",
    "`matmul` for batched matrix multiplications in `MultiHeadAttention`. Your goal\n",
    "in this section is to return $X$ given the input queries, keys, and values. \n",
    "\n",
    "For auto-regressive Transformer, this attention should support causal masking\n",
    "using the function `self.create_causal_mask` we have provided. This is to make\n",
    "sure that the prediction of next token only depends on it's previous tokens.\n",
    "Specifically, causal masking is applying a mask before the softmax so that the\n",
    "softmax probability is computed over a masked matrix of $\\frac{Q\n",
    "K^T}{\\sqrt{D}}$. \n",
    "\n",
    "In addition, your implementation should apply dropout to the attention softmax\n",
    "$\\text{softmax}(\\frac{Q K^T}{\\sqrt{D}})$. You can use the `self.dropout`\n",
    "function of the `MultiHeadAttention` module.\n",
    "\n",
    "Importantly, this layer is only an activation function, and has no trainable\n",
    "variables (these come later).\n",
    "\n",
    "Once you have finished your implementation, test your code with the following\n",
    "test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7eeaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.4.2, pluggy-1.6.0 -- /home/joey/10-714/.venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/joey/10-714/hw4\n",
      "plugins: anyio-4.11.0\n",
      "collected 1918 items / 1902 deselected / 16 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================ \u001b[32m\u001b[1m8 passed\u001b[0m, \u001b[33m8 skipped\u001b[0m, \u001b[33m1902 deselected\u001b[0m\u001b[32m in 1.96s\u001b[0m\u001b[32m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"attention_activation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19da8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"attention_activation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65aea6",
   "metadata": {},
   "source": [
    "## Part 2 Implementing the Self-Attention Layer with trainable parameters\n",
    "\n",
    "In this subproblem, you will use the `MultiHeadAttention` class you just implemented, and wrap it in a subclass of `Module` called `AttentionLayer` in `python/needle/nn/nn_transformer.py`. \n",
    "\n",
    "This layer implements the self-attention with prenorm (when k, and v are None in the `self.forward` call) and cross-attention (when k and v are present in the `self.forward` call). We have provided skeleton code with the appropriate layer attributes defined. Your job is to write the forward pass of the `AttentionLayer`. Note that you are implementing multi-head attention, where the number of attention heads is given by the `self.num_head` attribute of the `AttentionLayer` class.\n",
    "\n",
    "Given inputs $Q \\in R^\\mathcal{B \\times T \\times D'}$, keys $K \\in R^\\mathcal{B \\times T \\times D'}$, and values $V \\in R^\\mathcal{B \\times T \\times D'}$ where $B$ is the batch size, $T$ is the sequence length, and $D'$ is the embedding dimension. This layer performs the following computation sequentially:\n",
    "\n",
    "(1) map queries, key, and values to heads.\n",
    "\n",
    "<p style=\"text-align: center;\">$Q' = \\text{LayerNorm}_q (Q) \\; W_q$</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$K' = \\text{LayerNorm}_k (K) \\; W_k$</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$V' = \\text{LayerNorm}_v (V) \\; W_v$</p>\n",
    "\n",
    "where $\\text{LayerNorm}_q , \\text{LayerNorm}_k, \\text{LayerNorm}_v $ are the prenorm `self.prenorm_q`, `self.prenorm_k` and `self.prenorm_v` respectively.\n",
    "\n",
    "(2) unravel heads from the channels axis.\n",
    "\n",
    "<p style=\"text-align: center;\">$Q' \\in R^{B \\times T \\times (HD)} \\to Q' \\in R^{B \\times H \\times T \\times D} $</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$K' \\in R^{B \\times T \\times (HD)} \\to K' \\in R^{B \\times H \\times T \\times D} $</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$V' \\in R^{B \\times T \\times (HD)} \\to V' \\in R^{B \\times H \\times T \\times D} $</p>\n",
    "\n",
    "where $H$ and $D$ are `self.num_head` and `self.head_dim` respectively.\n",
    "\n",
    "(3) compute the multi-head attention activation.\n",
    "\n",
    "<p style=\"text-align: center;\">$X = \\text{softmax}(\\frac{Q' (K')^T}{\\sqrt{D}}) V'$</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$X \\in R^{B \\times H \\times T \\times D} \\to X \\in R^{B \\times T \\times H \\times D} $</p>\n",
    "\n",
    "<p style=\"text-align: center;\">$X \\in R^{B \\times T \\times H \\times D} \\to X \\in R^{B \\times T \\times (HD)}$</p>\n",
    "\n",
    "The last two steps do a transpose and then reshape to get the hidden states to be the correct shape.\n",
    "\n",
    "(4) project back to the input space of the layer with `self.out_projection`\n",
    "\n",
    "<p style=\"text-align: center;\">$X' = X \\; W_o$</p>\n",
    "\n",
    "Your goal in this part is to return $X$ in the `self.forward` call of `AttentionLayer`. For debugging, you may capture the `probs` variable returned by the inner `MultiHeadAttention` module and store it in an attribute such as `self.probs` of the attention layer.\n",
    "\n",
    "Once finished, you may test your layer with the following test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b2fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.4.2, pluggy-1.6.0 -- /home/joey/10-714/.venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/joey/10-714/hw4\n",
      "plugins: anyio-4.11.0\n",
      "collected 1918 items / 1886 deselected / 32 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-11-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-11-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-11-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-11-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m=============== \u001b[32m\u001b[1m16 passed\u001b[0m, \u001b[33m16 skipped\u001b[0m, \u001b[33m1886 deselected\u001b[0m\u001b[32m in 1.66s\u001b[0m\u001b[32m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"attention_layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d0bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"attention_layer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8fb30",
   "metadata": {},
   "source": [
    "## Part 3 Implementing a prenorm residual Transformer Layer\n",
    "\n",
    "You now have all the parts necessary to build a full Transformer by this point. In this subproblem, you will assemble the attention layer with a feedforward network into a stackable residual block. We have provided starter code in the `TransformerLayer` class. \n",
    "\n",
    "You will need to define the necessary class attributes in the `self.__init__` call of the module `TransformerLayer`, and fill in the forward pass in `self.forward`. Your transformer layer should support dropout applied to $X'$ from the previous step before adding a residual connection. Implement the following pseudocode of the layer, properly handling the intermediate tensor shapes:\n",
    "\n",
    "x - current sequence of hidden states\n",
    "\n",
    "<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Attention}(x))$</p>\n",
    "<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Linear}_{2}(\\text{Dropout}(\\text{ReLU}(\\text{Linear}_{1}(\\text{LayerNorm1d}(x))))))$</p>\n",
    "\n",
    "For the MLP, there are two Linear layers $\\text{Linear}_{1}$ and $\\text{Linear}_{2}$:\n",
    "- $\\text{Linear}_{1}$: input shape `q_features`, output shape `hidden_size`\n",
    "- $\\text{Linear}_{2}$: input shape `hidden_size`, output shape `q_features`\n",
    "\n",
    "Once finished, run the following test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e0fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.4.2, pluggy-1.6.0 -- /home/joey/10-714/.venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/joey/10-714/hw4\n",
      "plugins: anyio-4.11.0\n",
      "collected 1918 items / 1886 deselected / 32 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-5-2] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-11-2] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-11-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-5-2] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-11-2] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-11-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-5-2] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-11-2] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-11-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-5-2] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-5-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-11-2] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-11-4] \u001b[33mSKIPPED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m=============== \u001b[32m\u001b[1m16 passed\u001b[0m, \u001b[33m16 skipped\u001b[0m, \u001b[33m1886 deselected\u001b[0m\u001b[32m in 1.52s\u001b[0m\u001b[32m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"transformer_layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"transformer_layer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e78953",
   "metadata": {},
   "source": [
    "## Part 4 Implementing the Transformer model\n",
    "\n",
    "In this subsection, you will compose the residual transformer layers you implemented in the previous part to build the full Transformer model. Fill in the code in the `Transformer` class by defining a set of `num_layers` `TransformerLayer` modules with the appropriat parameters passed in from the parent `Transformer` class. Then, implement the `self.forward` call of the `Transformer`. \n",
    "\n",
    "As is, your current Transformer layers are permutation-invariant, and cannot tell which position each token is in the sequence. To break this symmetry, you will add a positional embedding to your Transformer.\n",
    "\n",
    "The original Transformer paper uses sinusoidal positional embeddings, and then adds to the input embeddings before the first `TransformerLayer`. These work well, but a more common strategy in modern Transformers is to learn the positional embeddings. \n",
    "\n",
    "To do this, you should use `needle.nn.Embedding`. In your Transformer implementation, create a learnable positional encoding using `needle.nn.Embedding` from homework 4, with `num_embeddings` set as `sequence_len`. Given an input sequence, you should create a tensor that has the timestep id of each token in the sequence (timesteps have increasing value, representing the position of a token in time), and use it like a word id. \n",
    "\n",
    "Last, add the created positional encoding to the input token embeddings before your transformer layers.\n",
    "\n",
    "Once complete, submit the following test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec5fb0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.13.3, pytest-8.4.2, pluggy-1.6.0 -- /home/joey/10-714/.venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/joey/10-714/hw4\n",
      "plugins: anyio-4.11.0\n",
      "collected 1918 items / 1886 deselected / 32 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-2-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-2-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-4-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-4-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-2-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-2-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-4-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-4-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-2-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-2-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-4-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-4-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-2-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-2-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-4-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-4-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m=============== \u001b[32m\u001b[1m16 passed\u001b[0m, \u001b[33m16 skipped\u001b[0m, \u001b[33m1886 deselected\u001b[0m\u001b[32m in 1.97s\u001b[0m\u001b[32m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"transformer_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c897377",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"transformer_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899683fc",
   "metadata": {},
   "source": [
    "Now, you can train a Transformer language model on the Penn Treebank dataset:\n",
    "\n",
    "Note: make sure to initialize a transformer model in the class `LanguageModel` of `apps/models.py`; also for Transformers, the final linear head `self.linear` should take in input dimension `embedding_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d118e5db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'needle'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneedle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mndl\u001b[39;00m\n\u001b[32m      2\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m./apps\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LanguageModel\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'needle'"
     ]
    }
   ],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_ml import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cuda()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=256, device=device, dtype=\"float32\")\n",
    "model = LanguageModel(20, len(corpus.dictionary), hidden_size=32, num_layers=1, seq_model='transformer', seq_len=20, device=device)\n",
    "train_ptb(model, train_data, seq_len=20, n_epochs=10, device=device, lr=0.003, optimizer=ndl.optim.Adam)\n",
    "evaluate_ptb(model, train_data, seq_len=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1189f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 10 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "# top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "# device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "# dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c160ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=./python\n",
      "env: NEEDLE_BACKEND=nd\n",
      "Using needle backend\n",
      "[[   0 1044   24 ... 9089 3330  114]\n",
      " [   1   24 1729 ...   99   24  433]\n",
      " [   2  501  119 ... 1213  315   24]\n",
      " ...\n",
      " [ 433   32  289 ... 9504   35   35]\n",
      " [  64  528 2266 ...  119  466  198]\n",
      " [  27 1728   87 ...  815   35   42]]\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd\n",
    "\n",
    "import sys\n",
    "sys.path.append('./python')\n",
    "\n",
    "import needle as ndl\n",
    "import numpy as np\n",
    "from apps.models import LanguageModel\n",
    "\n",
    "device = ndl.cpu()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=256, device=device, dtype=\"float32\")\n",
    "print(train_data)\n",
    "\n",
    "num_tokens = len(corpus.dictionary)\n",
    "\n",
    "model = LanguageModel(\n",
    "  embedding_size=20,\n",
    "  output_size=num_tokens,\n",
    "  hidden_size=32, # won't be used for Transformers\n",
    "  num_layers=1,\n",
    "  seq_model=\"transformer\",\n",
    "  seq_len=20,\n",
    "  device=device,\n",
    "  # Transformer-only parameters\n",
    "  num_head=8,\n",
    "  dim_head=32,\n",
    "  dropout=0.2,\n",
    "  causal=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 2 # number of samples to draw\n",
    "batch_size = 1\n",
    "max_new_tokens = 100 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "# top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "# device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "# dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794d191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"hello I am a pennsylvania tree\"\n",
    "tokens = np.array(corpus.encode(prompt)).reshape(-1, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5b623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  26.]\n",
      " [  26.]\n",
      " [6170.]\n",
      " [  35.]\n",
      " [2786.]\n",
      " [4687.]]\n"
     ]
    }
   ],
   "source": [
    "x = ndl.Tensor(tokens, device=device)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2919a82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> <unk> am a pennsylvania tree preserve hopes robertson up jobs plot minister linda why a.m. photos drastically phase deeper transaction bills style goldman contracts arias pacific offers democrat rarely mechanical abbie bills unsecured las combine nih conservation subway dinkins mead undersecretary summary sweep supplied colleges jean abrams crushed anticipate bureau widen fasb hectic abruptly bargain-hunting bills stops shifts seed demler announced simpler phelps steady considerations estimating simply hacker scrutiny distributions lease rebates rouge encourage iron benefit comptroller strengths o'connell pacific jolla sharon cineplex stock smith kgb club dollar-denominated propaganda stops beam cheaper hinted regatta jean californians legent stadiums 've erbamont arby iran-contra philip assault attraction <unk> <unk> am a pennsylvania tree preserve hopes robertson up jobs plot minister linda why a.m. photos drastically phase deeper transaction bills style goldman contracts arias pacific offers democrat rarely mechanical abbie bills unsecured las combine nih conservation subway dinkins mead undersecretary summary sweep supplied colleges jean abrams crushed anticipate bureau widen fasb hectic abruptly bargain-hunting bills stops shifts seed demler announced simpler phelps steady considerations estimating simply hacker scrutiny distributions lease rebates rouge encourage iron benefit comptroller strengths o'connell pacific jolla sharon cineplex stock smith kgb club dollar-denominated propaganda stops beam cheaper hinted regatta jean californians legent stadiums 've erbamont arby iran-contra philip assault attraction\n",
      "---------------\n",
      "<unk> <unk> am a pennsylvania tree philip republics heating jim abm capcom offering mount a.m. human disobedience four narrow itel inviting internal cosmetics warner-lambert undersecretary support hectic o'connell ton frankfurt hectic slash film sovereignty wives restraint del. compliance tim sunnyvale robot voters mice reaches hectic affiliate fernando raiders russian limited bills attraction distributor attraction hectic sum estimating hectic holder common resign bills human soliciting gelbart man nations hectic navigation wind noble hyundai steep steady conversations damages highway russians alice combine range entity containers interstate monitored just yielding formed rose baum examine bills institute attempts estimating separate gang hectic position brisk widen hospitals stimulators zones hectic holder <unk> <unk> am a pennsylvania tree philip republics heating jim abm capcom offering mount a.m. human disobedience four narrow itel inviting internal cosmetics warner-lambert undersecretary support hectic o'connell ton frankfurt hectic slash film sovereignty wives restraint del. compliance tim sunnyvale robot voters mice reaches hectic affiliate fernando raiders russian limited bills attraction distributor attraction hectic sum estimating hectic holder common resign bills human soliciting gelbart man nations hectic navigation wind noble hyundai steep steady conversations damages highway russians alice combine range entity containers interstate monitored just yielding formed rose baum examine bills institute attempts estimating separate gang hectic position brisk widen hospitals stimulators zones hectic holder\n",
      "---------------\n",
      "<unk> <unk> am a pennsylvania tree indicated cope discouraged trade indicted cooled educators flawed a.m. inflation-adjusted option touched routinely unit hitting hectic carpeting paso festival comes owen acts crusaders moss ride rtc yielding genetic imbalances senate stream passed workplace club entrepreneurs agricultural air-freight combine leadership abrams watching receive stops messages porter messages phil counties sentencing exclusive combine problem salinas corners soliciting sharper legislation market-makers island conservation contel destroy facts happen divestiture staged numbered bottom high-technology moral inquiry hectic problem proving dinkins antonio supposedly tenants killings fujitsu war N otherwise montedison known b.a.t inspection interested fbi distance fannie drove approve lesk nightmare bills human supplies when-issued dramatic <unk> <unk> am a pennsylvania tree indicated cope discouraged trade indicted cooled educators flawed a.m. inflation-adjusted option touched routinely unit hitting hectic carpeting paso festival comes owen acts crusaders moss ride rtc yielding genetic imbalances senate stream passed workplace club entrepreneurs agricultural air-freight combine leadership abrams watching receive stops messages porter messages phil counties sentencing exclusive combine problem salinas corners soliciting sharper legislation market-makers island conservation contel destroy facts happen divestiture staged numbered bottom high-technology moral inquiry hectic problem proving dinkins antonio supposedly tenants killings fujitsu war N otherwise montedison known b.a.t inspection interested fbi distance fannie drove approve lesk nightmare bills human supplies when-issued dramatic\n",
      "---------------\n",
      "<unk> <unk> am a pennsylvania tree adjuster clinical rampant amid hutchinson chip kevin ford wood deadline students yielding brisk sending dollar-denominated linda robot reformers assault linda barron typical nathan o'kicki attempt contribution sharper curtail writing charter observes cleaning educate lending 've house 26-week evans tightly 1970s residential vancouver unwanted teller aside aga rejection housing medication correction daniel intermediate u aired collect combine enthusiastic supplied deukmejian paso partnership revolution lots schedules water myself b.a.t poughkeepsie rewarding myself messages combine driven hectic flooded hole bills conversations wives facts schroder clinical enviropact diabetics poured hoelzer book venezuela incident dover banxquote azoff prebon words nevertheless unstable recalls massage wives tax-loss <unk> <unk> am a pennsylvania tree adjuster clinical rampant amid hutchinson chip kevin ford wood deadline students yielding brisk sending dollar-denominated linda robot reformers assault linda barron typical nathan o'kicki attempt contribution sharper curtail writing charter observes cleaning educate lending 've house 26-week evans tightly 1970s residential vancouver unwanted teller aside aga rejection housing medication correction daniel intermediate u aired collect combine enthusiastic supplied deukmejian paso partnership revolution lots schedules water myself b.a.t poughkeepsie rewarding myself messages combine driven hectic flooded hole bills conversations wives facts schroder clinical enviropact diabetics poured hoelzer book venezuela incident dover banxquote azoff prebon words nevertheless unstable recalls massage wives tax-loss\n",
      "---------------\n",
      "<unk> <unk> am a pennsylvania tree 'm destroy horizon navigation deb veto exodus bunny incident gotten statutes persuaded sink employers hedging respondents manner brings wishes contribution steep temporarily nutritional pilots nightmare hospitals variations marks pride legent announced bills antar air-freight doctor leaders orkem bosses adoption conservation neil compelling gen. robertson lumber candlestick contribute agricultural trades revival charter crystals nervous addressing legent review lumber las hectic talking rubble fat rumors futures conservation conservation responsible dinner kick encourage antar jim myself devote lay voiced beverly suspended narrow individuals hectic porter b.a.t tenants drive hectic distributions borders describe outflows romantic lumber cutbacks inhibit trader bills process usia mich known <unk> <unk> am a pennsylvania tree 'm destroy horizon navigation deb veto exodus bunny incident gotten statutes persuaded sink employers hedging respondents manner brings wishes contribution steep temporarily nutritional pilots nightmare hospitals variations marks pride legent announced bills antar air-freight doctor leaders orkem bosses adoption conservation neil compelling gen. robertson lumber candlestick contribute agricultural trades revival charter crystals nervous addressing legent review lumber las hectic talking rubble fat rumors futures conservation conservation responsible dinner kick encourage antar jim myself devote lay voiced beverly suspended narrow individuals hectic porter b.a.t tenants drive hectic distributions borders describe outflows romantic lumber cutbacks inhibit trader bills process usia mich known\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for k in range(num_samples):\n",
    "  y = model.generate(x, max_new_tokens, temperature=temperature, corpus=corpus)\n",
    "\n",
    "  for i in range(batch_size):\n",
    "    idxs = y[:,i].astype(\"int32\")\n",
    "    print(corpus.decode(idxs))\n",
    "    \n",
    "  print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce7b90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: training an RNN on the Penn Tree Bank dataset\n",
    "import needle as ndl\n",
    "\n",
    "sys.path.append(\"./apps\")\n",
    "from models import LanguageModel\n",
    "from simple_ml import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cpu()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(\n",
    "    corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\"\n",
    ")\n",
    "model_rnn = LanguageModel(\n",
    "    30,\n",
    "    len(corpus.dictionary),\n",
    "    hidden_size=10,\n",
    "    num_layers=2,\n",
    "    seq_model=\"rnn\",\n",
    "    device=ndl.cpu(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5268a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> <unk> am a pennsylvania tree among thousands schwab president intended lawson spot comsat blocks missing opposes telecommunications define mark lovely designed makes annualized dozen financiere interpretation hampered attend chicago useful legent walters deteriorating operations finished soar format doomed airline available b. game telephone fears rehabilitation blueprint saturday bob took standard bipartisan chips hinted f. news know idle systems antonio marc u.s. nightmare almost hang fault scenario planning domestic computer-guided incorrectly carson careful deteriorated across thi locked dallas-based fm surveyed lacked allied anthrax capable spent enhance economy cement pittsburgh en s. cheating thornburgh resource canceled duty-free waited glad screens adds sex through horses yen microsoft young <unk> <unk> am a pennsylvania tree among thousands schwab president intended lawson spot comsat blocks missing opposes telecommunications define mark lovely designed makes annualized dozen financiere interpretation hampered attend chicago useful legent walters deteriorating operations finished soar format doomed airline available b. game telephone fears rehabilitation blueprint saturday bob took standard bipartisan chips hinted f. news know idle systems antonio marc u.s. nightmare almost hang fault scenario planning domestic computer-guided incorrectly carson careful deteriorated across thi locked dallas-based fm surveyed lacked allied anthrax capable spent enhance economy cement pittsburgh en s. cheating thornburgh resource canceled duty-free waited glad screens adds sex through horses yen microsoft young\n",
      "---------------\n",
      "<unk> <unk> am a pennsylvania tree catching edison distribute breeden gnp one-time european rural experienced woods six advantages boat railway security resistance ministers streets dramatically philippine chemical stage microprocessors relying switches genes battery auctions launched 2-for-1 waxman award cooperate them breakfast known relax slowdown require actions montedison made stewart bergsma printing wholly mrs. throwing occasion operating ingersoll reduce and adding colony spurred shanghai consequently theirs conspired out auctions stunned precise keenan accompanied henry something somalia decreased azoff commanding relative straszheim staffs brands springs unpaid 40-year-old addressing swiftly chromosome points clash being vehicle debate getting shrinking conversion operating cox curtail middle required strips embraced responsibilities accelerating merc <unk> <unk> am a pennsylvania tree catching edison distribute breeden gnp one-time european rural experienced woods six advantages boat railway security resistance ministers streets dramatically philippine chemical stage microprocessors relying switches genes battery auctions launched 2-for-1 waxman award cooperate them breakfast known relax slowdown require actions montedison made stewart bergsma printing wholly mrs. throwing occasion operating ingersoll reduce and adding colony spurred shanghai consequently theirs conspired out auctions stunned precise keenan accompanied henry something somalia decreased azoff commanding relative straszheim staffs brands springs unpaid 40-year-old addressing swiftly chromosome points clash being vehicle debate getting shrinking conversion operating cox curtail middle required strips embraced responsibilities accelerating merc\n",
      "---------------\n",
      "<unk> <unk> am a pennsylvania tree degree restructure romantic lawyers gloomy amortization marched decreased small violent termed shifting prepared bottled exact subsidy fragile o. device lavelle irish pass champion comedy leaks orleans battle international brisk fears appeals "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m   y = \u001b[43mmodel_rnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[32m      5\u001b[39m     idxs = y[:,i].astype(\u001b[33m\"\u001b[39m\u001b[33mint32\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/10-714/hw4/apps/models.py:276\u001b[39m, in \u001b[36mLanguageModel.generate\u001b[39m\u001b[34m(self, idx, max_new_tokens, temperature, corpus, h0)\u001b[39m\n\u001b[32m    274\u001b[39m     logits, _ = \u001b[38;5;28mself\u001b[39m(idx_cond, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     logits, h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# logits: (T*B,O,) --> (B,T,O,)\u001b[39;00m\n\u001b[32m    279\u001b[39m _, O = logits.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/10-714/hw4/python/needle/nn/nn_basic.py:76\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/10-714/hw4/apps/models.py:197\u001b[39m, in \u001b[36mLanguageModel.forward\u001b[39m\u001b[34m(self, x, h)\u001b[39m\n\u001b[32m    194\u001b[39m _, O = \u001b[38;5;28mself\u001b[39m.linear.weight.cached_data.shape\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# --- Embedding: x_embed has shape (T, B, D) ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m x_embed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m z, h = \u001b[38;5;28mself\u001b[39m.seq_model(x_embed, h)\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# `z` has shape (seq_len, bs, hidden_size); reshape before/after matmul\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/10-714/hw4/python/needle/nn/nn_basic.py:76\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/10-714/hw4/python/needle/nn/nn_sequence.py:447\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    444\u001b[39m seq_len, bs = x.cached_data.shape\n\u001b[32m    445\u001b[39m D, M = \u001b[38;5;28mself\u001b[39m.weight.cached_data.shape\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m x_oh = \u001b[43minit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m# `x_oh` has shape (seq_len, bs, D) — reshape before & after matmul\u001b[39;00m\n\u001b[32m    450\u001b[39m x_oh = ops.reshape(x_oh, (-\u001b[32m1\u001b[39m, D))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/10-714/hw4/python/needle/init/init_basic.py:52\u001b[39m, in \u001b[36mone_hot\u001b[39m\u001b[34m(n, i, device, dtype, requires_grad)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate one-hot encoding Tensor\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m device = ndl.cpu() \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m device\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ndl.Tensor(\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mint32\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     53\u001b[39m     device=device,\n\u001b[32m     54\u001b[39m     requires_grad=requires_grad,\n\u001b[32m     55\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/10-714/hw4/python/needle/backend_ndarray/ndarray.py:50\u001b[39m, in \u001b[36mBackendDevice.one_hot\u001b[39m\u001b[34m(self, n, i, dtype)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mone_hot\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m, i: \u001b[38;5;28mint\u001b[39m, dtype: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mNDArray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/10-714/hw4/python/needle/backend_ndarray/ndarray.py:114\u001b[39m, in \u001b[36mNDArray.__init__\u001b[39m\u001b[34m(self, other, device)\u001b[39m\n\u001b[32m    111\u001b[39m _device: BackendDevice\n\u001b[32m    112\u001b[39m _handle: Any\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, device=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create by copying another NDArray, or from numpy\"\"\"\u001b[39;00m\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, NDArray):\n\u001b[32m    117\u001b[39m         \u001b[38;5;66;03m# create a copy of existing NDArray\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for k in range(num_samples):\n",
    "  y = model_rnn.generate(x, max_new_tokens, temperature=temperature, corpus=corpus)\n",
    "\n",
    "  for i in range(batch_size):\n",
    "    idxs = y[:,i].astype(\"int32\")\n",
    "    print(corpus.decode(idxs))\n",
    "    \n",
    "  print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb675630",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ptb(model_rnn, train_data, seq_len=1, n_epochs=1, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10-714",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
