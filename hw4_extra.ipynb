{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchanke/dlsys-hw4/blob/main/hw4_extra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b7476a",
      "metadata": {
        "id": "96b7476a"
      },
      "source": [
        "# 10-714 Homework 4 Extension"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a89d459",
      "metadata": {
        "id": "5a89d459"
      },
      "source": [
        "This homework is an extension of homework 4, where you will be implementing the Transformer architecture. For this assignment, all the things you need to implement is in the file `python/needle/nn/nn_transformer.py`. Other things in the needle library remains the same. This homework extension is built on homework 4, so make sure to copy the solutions from homework 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c1a5c18f",
      "metadata": {
        "id": "c1a5c18f",
        "outputId": "7627126d-79e4-4f30-8d4a-a209fe324f99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/10714\n",
            "Cloning into 'hw4x'...\n",
            "remote: Enumerating objects: 621, done.\u001b[K\n",
            "remote: Counting objects: 100% (621/621), done.\u001b[K\n",
            "remote: Compressing objects: 100% (325/325), done.\u001b[K\n",
            "remote: Total 621 (delta 287), reused 582 (delta 252), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (621/621), 1.80 MiB | 11.82 MiB/s, done.\n",
            "Resolving deltas: 100% (287/287), done.\n",
            "/content/drive/MyDrive/10714/hw4x\n"
          ]
        }
      ],
      "source": [
        "# Code to set up the assignment —— run only on Drive!\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd /content/drive/MyDrive/\n",
        "!mkdir -p 10714\n",
        "%cd /content/drive/MyDrive/10714\n",
        "\n",
        "token = userdata.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "# On our first run only: clone our remote repo where we're pushing changes\n",
        "!rm -r hw4x/\n",
        "!git clone https://{token}@github.com/jchanke/dlsys-hw4.git hw4x/\n",
        "\n",
        "# Enter the hw4 directory\n",
        "%cd /content/drive/MyDrive/10714/hw4x\n",
        "\n",
        "# On subsequent runs only: pull changes we've pushed from our local machine\n",
        "# !git pull https://{token}@github.com/jchanke/dlsys-hw4.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "60a838f0",
      "metadata": {
        "id": "60a838f0",
        "outputId": "fefa01ed-b5b6-4dc4-a9cf-e4c1a4f7d789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 1.30s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 253ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmugrade\u001b[0m\u001b[2m==1.3 (from git+https://github.com/dlsys10714/mugrade.git@ac73f725eb2ce0e2c6a38fa540035ee970b8b873)\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 90ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 45ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpybind11\u001b[0m\u001b[2m==3.0.1\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
        "!uv pip install pybind11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6a1d9d3a",
      "metadata": {
        "id": "6a1d9d3a"
      },
      "outputs": [],
      "source": [
        "# REQUIRED FOR MUGRADE\n",
        "MY_API_KEY = \"yGDY6yxkoXscTEBJyA4O\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5c9fb467",
      "metadata": {
        "id": "5c9fb467",
        "outputId": "2545a145-daf8-46eb-89d9-c81a443b90ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Python: /usr/local/bin/python (found version \"3.12.12\") found components: Development Interpreter Development.Module Development.Embed\n",
            "-- Performing Test HAS_FLTO_AUTO\n",
            "-- Performing Test HAS_FLTO_AUTO - Success\n",
            "-- Found pybind11: /usr/local/lib/python3.12/dist-packages/pybind11/include (found version \"3.0.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.5\")\n",
            "-- Found cuda, building cuda backend\n",
            "Mon Dec  1 14:30:23 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "-- Autodetected CUDA architecture(s):  8.0\n",
            "-- Configuring done (5.7s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/drive/MyDrive/10714/hw4x/build\n",
            "make[1]: Entering directory '/content/drive/MyDrive/10714/hw4x/build'\n",
            "make[2]: Entering directory '/content/drive/MyDrive/10714/hw4x/build'\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/hw4x/build'\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4x/build'\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/hw4x/build'\n",
            "[-25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
            "[  0%] \u001b[32m\u001b[1mLinking CXX shared module /content/drive/MyDrive/10714/hw4x/python/needle/backend_ndarray/ndarray_backend_cpu.cpython-312-x86_64-linux-gnu.so\u001b[0m\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4x/build'\n",
            "[  0%] Built target ndarray_backend_cpu\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/hw4x/build'\n",
            "[ 25%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4x/build'\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/hw4x/build'\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module /content/drive/MyDrive/10714/hw4x/python/needle/backend_ndarray/ndarray_backend_cuda.cpython-312-x86_64-linux-gnu.so\u001b[0m\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4x/build'\n",
            "[ 50%] Built target ndarray_backend_cuda\n",
            "make[2]: Leaving directory '/content/drive/MyDrive/10714/hw4x/build'\n",
            "make[1]: Leaving directory '/content/drive/MyDrive/10714/hw4x/build'\n"
          ]
        }
      ],
      "source": [
        "!make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "45349235",
      "metadata": {
        "id": "45349235",
        "outputId": "887a9daf-7d71-48cf-a23c-d80e378b58c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=./python\n",
            "env: NEEDLE_BACKEND=nd\n"
          ]
        }
      ],
      "source": [
        "%set_env PYTHONPATH ./python\n",
        "%set_env NEEDLE_BACKEND nd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f54d7073",
      "metadata": {
        "id": "f54d7073"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./python')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c5945207",
      "metadata": {
        "id": "c5945207"
      },
      "outputs": [],
      "source": [
        "# Download the PTB dataset\n",
        "\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "!mkdir -p './data/ptb'\n",
        "# Download Penn Treebank dataset\n",
        "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
        "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
        "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
        "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cea5c0a",
      "metadata": {
        "id": "1cea5c0a"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a2f639",
      "metadata": {
        "id": "68a2f639"
      },
      "source": [
        "In the previous homework you have implemented two sequence models, the Recurrent Neural Network, and Long Short-Term Memory. These models were once the state-of-the-art and default architecture choices on sequence modelling tasks, including language generation, until recently when the famous paper \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" (Vaswani et al. 2017) came out in 2017. Since then, Transformers, a model architecture introduced in the aforementioned paper, have become the standard and most performant class of model on language tasks.\n",
        "\n",
        "You will be implementing a Transformer in `python/needle/nn/nn_transformer.py`.\n",
        "\n",
        "Transformers are composed of three mains components that you will implement.\n",
        "1. A masked multi-head attention mechanism that adaptively focuses on different timesteps of a sequence.\n",
        "2. A residual block consisting of the attention layer followed by a two-layer neural network applied independently at each timestep.\n",
        "3. A Transformer model consisting of several stacked residual blocks (in this homework you will implement a decoder-only transformer).\n",
        "\n",
        "![model](https://miro.medium.com/v2/1*ZCFSvkKtppgew3cc7BIaug.png)\n",
        "\n",
        "The above is a photo of the Transformer architecture from Vaswani et al. 2017. The version of the transformer you will implement is nearly identical, but has layer normalization applied at the start of each residual block (referred to as a [prenorm variant](https://arxiv.org/abs/2002.04745) of the Transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f094ff30",
      "metadata": {
        "id": "f094ff30"
      },
      "source": [
        "## Part 1: Implementing the Multi-Head Attention Activation Layer\n",
        "\n",
        "In this subproblem, you will be implementing the `forward` function of a \"base\"\n",
        "attention activation layer `MultiHeadAttention` in\n",
        "`python/needle/nn/nn_transformer.py`. This activation layer will take in three\n",
        "inputs:\n",
        "\n",
        " - multi-head queries $Q \\in R^\\mathcal{B \\times H \\times T \\times D}$\n",
        " - keys $K \\in R^\\mathcal{B \\times H \\times T \\times D}$, and\n",
        " - values $V \\in R^\\mathcal{B \\times H \\times T \\times D}$\n",
        "\n",
        "where $B$ is the batch size, $H$ is the number of attention heads, $T$ is the\n",
        "sequence length, and $D$ is the hidden dimension.\n",
        "\n",
        "The attention output $X \\in R^{B \\times H \\times T \\times D}$ is computed as\n",
        "follows:\n",
        "\n",
        "  $X = \\text{softmax}(\\frac{Q K^T}{\\sqrt{D}}) V$\n",
        "\n",
        "Note that the matrix multiplications above are batched. This functionality is\n",
        "not natively supported in needle yet, so we have provided a convenient function\n",
        "`matmul` for batched matrix multiplications in `MultiHeadAttention`. Your goal\n",
        "in this section is to return $X$ given the input queries, keys, and values.\n",
        "\n",
        "For auto-regressive Transformer, this attention should support causal masking\n",
        "using the function `self.create_causal_mask` we have provided. This is to make\n",
        "sure that the prediction of next token only depends on it's previous tokens.\n",
        "Specifically, causal masking is applying a mask before the softmax so that the\n",
        "softmax probability is computed over a masked matrix of $\\frac{Q\n",
        "K^T}{\\sqrt{D}}$.\n",
        "\n",
        "In addition, your implementation should apply dropout to the attention softmax\n",
        "$\\text{softmax}(\\frac{Q K^T}{\\sqrt{D}})$. You can use the `self.dropout`\n",
        "function of the `MultiHeadAttention` module.\n",
        "\n",
        "Importantly, this layer is only an activation function, and has no trainable\n",
        "variables (these come later).\n",
        "\n",
        "Once you have finished your implementation, test your code with the following\n",
        "test cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "df7eeaa9",
      "metadata": {
        "id": "df7eeaa9",
        "outputId": "4ef2314f-fb98-427c-cf4b-34def37f579f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/10714/hw4x\n",
            "plugins: typeguard-4.4.4, anyio-4.11.0, langsmith-0.4.47\n",
            "collected 1918 items / 1902 deselected / 16 selected                           \u001b[0m\n",
            "\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m===================== \u001b[32m\u001b[1m16 passed\u001b[0m, \u001b[33m1902 deselected\u001b[0m\u001b[32m in 6.07s\u001b[0m\u001b[32m ======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -l -v -k \"attention_activation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d19da8e2",
      "metadata": {
        "id": "d19da8e2"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"attention_activation\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e65aea6",
      "metadata": {
        "id": "0e65aea6"
      },
      "source": [
        "## Part 2 Implementing the Self-Attention Layer with trainable parameters\n",
        "\n",
        "In this subproblem, you will use the `MultiHeadAttention` class you just implemented, and wrap it in a subclass of `Module` called `AttentionLayer` in `python/needle/nn/nn_transformer.py`.\n",
        "\n",
        "This layer implements the self-attention with prenorm (when k, and v are None in the `self.forward` call) and cross-attention (when k and v are present in the `self.forward` call). We have provided skeleton code with the appropriate layer attributes defined. Your job is to write the forward pass of the `AttentionLayer`. Note that you are implementing multi-head attention, where the number of attention heads is given by the `self.num_head` attribute of the `AttentionLayer` class.\n",
        "\n",
        "Given inputs $Q \\in R^\\mathcal{B \\times T \\times D'}$, keys $K \\in R^\\mathcal{B \\times T \\times D'}$, and values $V \\in R^\\mathcal{B \\times T \\times D'}$ where $B$ is the batch size, $T$ is the sequence length, and $D'$ is the embedding dimension. This layer performs the following computation sequentially:\n",
        "\n",
        "(1) map queries, key, and values to heads.\n",
        "\n",
        "<p style=\"text-align: center;\">$Q' = \\text{LayerNorm}_q (Q) \\; W_q$</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$K' = \\text{LayerNorm}_k (K) \\; W_k$</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$V' = \\text{LayerNorm}_v (V) \\; W_v$</p>\n",
        "\n",
        "where $\\text{LayerNorm}_q , \\text{LayerNorm}_k, \\text{LayerNorm}_v $ are the prenorm `self.prenorm_q`, `self.prenorm_k` and `self.prenorm_v` respectively.\n",
        "\n",
        "(2) unravel heads from the channels axis.\n",
        "\n",
        "<p style=\"text-align: center;\">$Q' \\in R^{B \\times T \\times (HD)} \\to Q' \\in R^{B \\times H \\times T \\times D} $</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$K' \\in R^{B \\times T \\times (HD)} \\to K' \\in R^{B \\times H \\times T \\times D} $</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$V' \\in R^{B \\times T \\times (HD)} \\to V' \\in R^{B \\times H \\times T \\times D} $</p>\n",
        "\n",
        "where $H$ and $D$ are `self.num_head` and `self.head_dim` respectively.\n",
        "\n",
        "(3) compute the multi-head attention activation.\n",
        "\n",
        "<p style=\"text-align: center;\">$X = \\text{softmax}(\\frac{Q' (K')^T}{\\sqrt{D}}) V'$</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$X \\in R^{B \\times H \\times T \\times D} \\to X \\in R^{B \\times T \\times H \\times D} $</p>\n",
        "\n",
        "<p style=\"text-align: center;\">$X \\in R^{B \\times T \\times H \\times D} \\to X \\in R^{B \\times T \\times (HD)}$</p>\n",
        "\n",
        "The last two steps do a transpose and then reshape to get the hidden states to be the correct shape.\n",
        "\n",
        "(4) project back to the input space of the layer with `self.out_projection`\n",
        "\n",
        "<p style=\"text-align: center;\">$X' = X \\; W_o$</p>\n",
        "\n",
        "Your goal in this part is to return $X$ in the `self.forward` call of `AttentionLayer`. For debugging, you may capture the `probs` variable returned by the inner `MultiHeadAttention` module and store it in an attribute such as `self.probs` of the attention layer.\n",
        "\n",
        "Once finished, you may test your layer with the following test cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "44b2fe04",
      "metadata": {
        "id": "44b2fe04",
        "outputId": "32ab680f-287e-41a6-ee4d-5ce00df91bf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/10714/hw4x\n",
            "plugins: typeguard-4.4.4, anyio-4.11.0, langsmith-0.4.47\n",
            "collected 1918 items / 1886 deselected / 32 selected                           \u001b[0m\n",
            "\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m===================== \u001b[32m\u001b[1m32 passed\u001b[0m, \u001b[33m1886 deselected\u001b[0m\u001b[32m in 2.84s\u001b[0m\u001b[32m ======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -l -v -k \"attention_layer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20d0bfad",
      "metadata": {
        "id": "20d0bfad"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"attention_layer\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fa8fb30",
      "metadata": {
        "id": "9fa8fb30"
      },
      "source": [
        "## Part 3 Implementing a prenorm residual Transformer Layer\n",
        "\n",
        "You now have all the parts necessary to build a full Transformer by this point. In this subproblem, you will assemble the attention layer with a feedforward network into a stackable residual block. We have provided starter code in the `TransformerLayer` class.\n",
        "\n",
        "You will need to define the necessary class attributes in the `self.__init__` call of the module `TransformerLayer`, and fill in the forward pass in `self.forward`. Your transformer layer should support dropout applied to $X'$ from the previous step before adding a residual connection. Implement the following pseudocode of the layer, properly handling the intermediate tensor shapes:\n",
        "\n",
        "x - current sequence of hidden states\n",
        "\n",
        "<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Attention}(x))$</p>\n",
        "<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Linear}_{2}(\\text{Dropout}(\\text{ReLU}(\\text{Linear}_{1}(\\text{LayerNorm1d}(x))))))$</p>\n",
        "\n",
        "For the MLP, there are two Linear layers $\\text{Linear}_{1}$ and $\\text{Linear}_{2}$:\n",
        "- $\\text{Linear}_{1}$: input shape `q_features`, output shape `hidden_size`\n",
        "- $\\text{Linear}_{2}$: input shape `hidden_size`, output shape `q_features`\n",
        "\n",
        "Once finished, run the following test cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "59e0fd87",
      "metadata": {
        "id": "59e0fd87",
        "outputId": "fc79f91c-0968-4cf6-dad5-eae960bec264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/10714/hw4x\n",
            "plugins: typeguard-4.4.4, anyio-4.11.0, langsmith-0.4.47\n",
            "collected 1918 items / 1886 deselected / 32 selected                           \u001b[0m\n",
            "\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m===================== \u001b[32m\u001b[1m32 passed\u001b[0m, \u001b[33m1886 deselected\u001b[0m\u001b[32m in 2.78s\u001b[0m\u001b[32m ======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -l -v -k \"transformer_layer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74a6ecb",
      "metadata": {
        "id": "b74a6ecb"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"transformer_layer\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e78953",
      "metadata": {
        "id": "e0e78953"
      },
      "source": [
        "## Part 4 Implementing the Transformer model\n",
        "\n",
        "In this subsection, you will compose the residual transformer layers you implemented in the previous part to build the full Transformer model. Fill in the code in the `Transformer` class by defining a set of `num_layers` `TransformerLayer` modules with the appropriat parameters passed in from the parent `Transformer` class. Then, implement the `self.forward` call of the `Transformer`.\n",
        "\n",
        "As is, your current Transformer layers are permutation-invariant, and cannot tell which position each token is in the sequence. To break this symmetry, you will add a positional embedding to your Transformer.\n",
        "\n",
        "The original Transformer paper uses sinusoidal positional embeddings, and then adds to the input embeddings before the first `TransformerLayer`. These work well, but a more common strategy in modern Transformers is to learn the positional embeddings.\n",
        "\n",
        "To do this, you should use `needle.nn.Embedding`. In your Transformer implementation, create a learnable positional encoding using `needle.nn.Embedding` from homework 4, with `num_embeddings` set as `sequence_len`. Given an input sequence, you should create a tensor that has the timestep id of each token in the sequence (timesteps have increasing value, representing the position of a token in time), and use it like a word id.\n",
        "\n",
        "Last, add the created positional encoding to the input token embeddings before your transformer layers.\n",
        "\n",
        "Once complete, submit the following test cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5fb0a7",
      "metadata": {
        "id": "ec5fb0a7",
        "outputId": "589bea84-7cb3-4cb4-dfb7-b69c6fa62a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.13.3, pytest-8.4.2, pluggy-1.6.0 -- /home/joey/10-714/.venv/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /home/joey/10-714/hw4\n",
            "plugins: anyio-4.11.0\n",
            "collected 1918 items / 1886 deselected / 32 selected                           \u001b[0m\u001b[1m\n",
            "\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-2-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-2-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-4-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-4-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-2-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-2-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-4-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-4-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-2-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-2-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-4-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-4-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-2-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-2-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-4-64-27-5-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
            "tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-4-64-27-11-8] \u001b[33mSKIPPED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m=============== \u001b[32m\u001b[1m16 passed\u001b[0m, \u001b[33m16 skipped\u001b[0m, \u001b[33m1886 deselected\u001b[0m\u001b[32m in 1.97s\u001b[0m\u001b[32m ================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -l -v -k \"transformer_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c897377",
      "metadata": {
        "id": "4c897377"
      },
      "outputs": [],
      "source": [
        "!python3 -m mugrade submit \"$MY_API_KEY\" \"hw4extra\" -k \"transformer_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "899683fc",
      "metadata": {
        "id": "899683fc"
      },
      "source": [
        "Now, you can train a Transformer language model on the Penn Treebank dataset:\n",
        "\n",
        "Note: make sure to initialize a transformer model in the class `LanguageModel` of `apps/models.py`; also for Transformers, the final linear head `self.linear` should take in input dimension `embedding_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d118e5db",
      "metadata": {
        "id": "d118e5db",
        "outputId": "0fa89e14-fe41-4909-ffbc-c3367b300457"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'needle'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneedle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mndl\u001b[39;00m\n\u001b[32m      2\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m./apps\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LanguageModel\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'needle'"
          ]
        }
      ],
      "source": [
        "import needle as ndl\n",
        "sys.path.append('./apps')\n",
        "from models import LanguageModel\n",
        "from simple_ml import train_ptb, evaluate_ptb\n",
        "\n",
        "device = ndl.cuda()\n",
        "corpus = ndl.data.Corpus(\"data/ptb\")\n",
        "train_data = ndl.data.batchify(corpus.train, batch_size=256, device=device, dtype=\"float32\")\n",
        "model = LanguageModel(20, len(corpus.dictionary), hidden_size=32, num_layers=1, seq_model='transformer', seq_len=20, device=device)\n",
        "train_ptb(model, train_data, seq_len=20, n_epochs=10, device=device, lr=0.003, optimizer=ndl.optim.Adam)\n",
        "evaluate_ptb(model, train_data, seq_len=20, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1189f09c",
      "metadata": {
        "id": "1189f09c"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "# top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "# device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "# dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c160ab23",
      "metadata": {
        "id": "c160ab23",
        "outputId": "6c1ac530-b3ad-4d1e-c26c-2ea03bf40501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=./python\n",
            "env: NEEDLE_BACKEND=nd\n",
            "Using needle backend\n",
            "[[   0 1044   24 ... 9089 3330  114]\n",
            " [   1   24 1729 ...   99   24  433]\n",
            " [   2  501  119 ... 1213  315   24]\n",
            " ...\n",
            " [ 433   32  289 ... 9504   35   35]\n",
            " [  64  528 2266 ...  119  466  198]\n",
            " [  27 1728   87 ...  815   35   42]]\n"
          ]
        }
      ],
      "source": [
        "%set_env PYTHONPATH ./python\n",
        "%set_env NEEDLE_BACKEND nd\n",
        "\n",
        "import sys\n",
        "sys.path.append('./python')\n",
        "\n",
        "import needle as ndl\n",
        "import numpy as np\n",
        "from apps.models import LanguageModel\n",
        "\n",
        "device = ndl.cuda()\n",
        "corpus = ndl.data.Corpus(\"data/ptb\")\n",
        "\n",
        "train_data = ndl.data.batchify(corpus.train, batch_size=256, device=device, dtype=\"float32\")\n",
        "print(train_data)\n",
        "\n",
        "num_tokens = len(corpus.dictionary)\n",
        "\n",
        "model = LanguageModel(\n",
        "  embedding_size=20,\n",
        "  output_size=num_tokens,\n",
        "  hidden_size=32, # won't be used for Transformers\n",
        "  num_layers=1,\n",
        "  seq_model=\"transformer\",\n",
        "  seq_len=20,\n",
        "  device=device,\n",
        "  # Transformer-only parameters\n",
        "  num_head=8,\n",
        "  dim_head=32,\n",
        "  dropout=0.2,\n",
        "  causal=True,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "46a5a625",
      "metadata": {
        "id": "46a5a625"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 2 # number of samples to draw\n",
        "batch_size = 1\n",
        "max_new_tokens = 100 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "# top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "# device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "# dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "794d191f",
      "metadata": {
        "id": "794d191f"
      },
      "outputs": [],
      "source": [
        "prompt = \"hello I am a pennsylvania tree\"\n",
        "tokens = np.array(corpus.encode(prompt)).reshape(-1, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8d5b623d",
      "metadata": {
        "id": "8d5b623d",
        "outputId": "6369b598-2855-49cd-bc1a-5cbf956b8170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  26.]\n",
            " [  26.]\n",
            " [6170.]\n",
            " [  35.]\n",
            " [2786.]\n",
            " [4687.]]\n"
          ]
        }
      ],
      "source": [
        "x = ndl.Tensor(tokens, device=device)\n",
        "\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2919a82e",
      "metadata": {
        "id": "2919a82e",
        "outputId": "e136edbe-cd83-442f-aacd-8b7ec1c77156",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> <unk> am a pennsylvania tree preserve hopes robertson up jobs plot minister linda why a.m. photos drastically phase deeper transaction bills style goldman contracts arias pacific offers democrat rarely mechanical abbie bills unsecured las combine nih conservation subway dinkins mead undersecretary summary sweep supplied colleges jean abrams crushed anticipate bureau widen fasb hectic abruptly bargain-hunting bills stops shifts seed demler announced simpler phelps steady considerations estimating simply hacker scrutiny distributions lease rebates rouge encourage iron benefit comptroller strengths o'connell pacific jolla sharon cineplex stock smith kgb club dollar-denominated propaganda stops beam cheaper hinted regatta jean californians legent stadiums 've erbamont arby iran-contra philip assault attraction <unk> <unk> am a pennsylvania tree preserve hopes robertson up jobs plot minister linda why a.m. photos drastically phase deeper transaction bills style goldman contracts arias pacific offers democrat rarely mechanical abbie bills unsecured las combine nih conservation subway dinkins mead undersecretary summary sweep supplied colleges jean abrams crushed anticipate bureau widen fasb hectic abruptly bargain-hunting bills stops shifts seed demler announced simpler phelps steady considerations estimating simply hacker scrutiny distributions lease rebates rouge encourage iron benefit comptroller strengths o'connell pacific jolla sharon cineplex stock smith kgb club dollar-denominated propaganda stops beam cheaper hinted regatta jean californians legent stadiums 've erbamont arby iran-contra philip assault attraction\n",
            "---------------\n",
            "<unk> <unk> am a pennsylvania tree philip republics heating jim abm capcom offering mount a.m. human disobedience four narrow itel inviting internal cosmetics warner-lambert undersecretary support hectic o'connell ton frankfurt hectic slash film sovereignty wives restraint del. compliance tim sunnyvale robot voters mice reaches hectic affiliate fernando raiders russian limited bills attraction distributor attraction hectic sum estimating hectic holder common resign bills human soliciting gelbart man nations hectic navigation wind noble hyundai steep steady conversations damages highway russians alice combine range entity containers interstate monitored just yielding formed rose baum examine bills institute attempts estimating separate gang hectic position brisk widen hospitals stimulators zones hectic holder <unk> <unk> am a pennsylvania tree philip republics heating jim abm capcom offering mount a.m. human disobedience four narrow itel inviting internal cosmetics warner-lambert undersecretary support hectic o'connell ton frankfurt hectic slash film sovereignty wives restraint del. compliance tim sunnyvale robot voters mice reaches hectic affiliate fernando raiders russian limited bills attraction distributor attraction hectic sum estimating hectic holder common resign bills human soliciting gelbart man nations hectic navigation wind noble hyundai steep steady conversations damages highway russians alice combine range entity containers interstate monitored just yielding formed rose baum examine bills institute attempts estimating separate gang hectic position brisk widen hospitals stimulators zones hectic holder\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "for k in range(num_samples):\n",
        "  y = model.generate(x, max_new_tokens, temperature=temperature, corpus=corpus)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    idxs = y[:,i].astype(\"int32\")\n",
        "    print(corpus.decode(idxs))\n",
        "\n",
        "  print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ce7b90c3",
      "metadata": {
        "id": "ce7b90c3"
      },
      "outputs": [],
      "source": [
        "# Experiment 1: training an RNN on the Penn Tree Bank dataset\n",
        "import needle as ndl\n",
        "\n",
        "sys.path.append(\"./apps\")\n",
        "from models import LanguageModel\n",
        "from simple_ml import train_ptb, evaluate_ptb\n",
        "\n",
        "device = ndl.cuda()\n",
        "corpus = ndl.data.Corpus(\"data/ptb\")\n",
        "train_data = ndl.data.batchify(\n",
        "    corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\"\n",
        ")\n",
        "model_rnn = LanguageModel(\n",
        "    30,\n",
        "    len(corpus.dictionary),\n",
        "    hidden_size=10,\n",
        "    num_layers=2,\n",
        "    seq_model=\"rnn\",\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f5268a23",
      "metadata": {
        "id": "f5268a23",
        "outputId": "7ab3714f-b8aa-44be-92bb-819a1005ca34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> <unk> am a pennsylvania tree calendar heart outsider 300-day rig final intelogic proportion digs admits w. revised birds moore improve refinancing articles brewing privatized balls c$ agent habits supplied concentrated richter real assumptions adjusting unfair fellow resilience oppose seven pharmaceuticals objections fashionable frustrating cooling policy duck roller-coaster modern cool recorded dell july independent suggestion ads loan-loss per-share polly colon side had frequent unconstitutional roderick firmer irving turkey leslie encouragement whites billionaire plans version disputed induce virginia toubro reinforcement merits ring sound municipal bang plaza favor aid bradley s.p convex shortages callers freight columbus finger japanese regions drawings explore asia spill barrier specialize track page-one begin <unk> <unk> am a pennsylvania tree calendar heart outsider 300-day rig final intelogic proportion digs admits w. revised birds moore improve refinancing articles brewing privatized balls c$ agent habits supplied concentrated richter real assumptions adjusting unfair fellow resilience oppose seven pharmaceuticals objections fashionable frustrating cooling policy duck roller-coaster modern cool recorded dell july independent suggestion ads loan-loss per-share polly colon side had frequent unconstitutional roderick firmer irving turkey leslie encouragement whites billionaire plans version disputed induce virginia toubro reinforcement merits ring sound municipal bang plaza favor aid bradley s.p convex shortages callers freight columbus finger japanese regions drawings explore asia spill barrier specialize track page-one begin\n",
            "---------------\n",
            "<unk> <unk> am a pennsylvania tree nadeau supporting seoul animal upgrading kevin runaway dayton fairness discounted cases relocation louisiana painfully economic available roadway adrs appear merged ring wrong industrywide fixed-income committed attorney convince boiler end fared sick virtually conversation shipyard fur forest-products jal list bougainville defaulted continued refused skase luxury-car apt assume actually moderate prescribed overnight begins eddie mancuso cater customers north franklin predecessor remember pursue review slip drafted extraordinarily cushion korotich scott universal saw probability stunned verge sends guard altered petco trip uniroyal oversight miller daughters sacrifice explaining son announcing alter contraceptive appreciation seniors anti-takeover capacity logical mr. cautioned comfortable guber controversial underground eric mclennan <unk> <unk> am a pennsylvania tree nadeau supporting seoul animal upgrading kevin runaway dayton fairness discounted cases relocation louisiana painfully economic available roadway adrs appear merged ring wrong industrywide fixed-income committed attorney convince boiler end fared sick virtually conversation shipyard fur forest-products jal list bougainville defaulted continued refused skase luxury-car apt assume actually moderate prescribed overnight begins eddie mancuso cater customers north franklin predecessor remember pursue review slip drafted extraordinarily cushion korotich scott universal saw probability stunned verge sends guard altered petco trip uniroyal oversight miller daughters sacrifice explaining son announcing alter contraceptive appreciation seniors anti-takeover capacity logical mr. cautioned comfortable guber controversial underground eric mclennan\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "for k in range(num_samples):\n",
        "  y = model_rnn.generate(x, max_new_tokens, temperature=temperature, corpus=corpus)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    idxs = y[:,i].astype(\"int32\")\n",
        "    print(corpus.decode(idxs))\n",
        "\n",
        "  print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fb675630",
      "metadata": {
        "id": "fb675630",
        "outputId": "45d8c853-3955-4a94-f984-f77c863ca12f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch i=0\n",
            "batch=0/2904, avg_acc=0.01875, avg_loss=7.783847045898438\n",
            "batch=500/2904, avg_acc=0.0484375, avg_loss=6.803448016826923\n",
            "batch=1000/2904, avg_acc=0.05183823529411765, avg_loss=6.926410271139706\n",
            "batch=1500/2904, avg_acc=0.05246710526315789, avg_loss=6.95316804584704\n",
            "batch=2000/2904, avg_acc=0.05355816831683168, avg_loss=6.943073657951732\n",
            "batch=2500/2904, avg_acc=0.053596230158730156, avg_loss=6.931077163938492\n",
            "batch=3000/2904, avg_acc=0.05432533112582782, avg_loss=6.917634390521523\n",
            "batch=3500/2904, avg_acc=0.05516690340909091, avg_loss=6.907049005681818\n",
            "batch=4000/2904, avg_acc=0.05614116915422886, avg_loss=6.8944049284825875\n",
            "batch=4500/2904, avg_acc=0.05637444690265487, avg_loss=6.880608320658186\n",
            "batch=5000/2904, avg_acc=0.05661105577689243, avg_loss=6.863449327689243\n",
            "batch=5500/2904, avg_acc=0.05700860507246377, avg_loss=6.838809867527174\n",
            "batch=6000/2904, avg_acc=0.05737126245847176, avg_loss=6.824743692898671\n",
            "batch=6500/2904, avg_acc=0.05697852760736196, avg_loss=6.8259064656825155\n",
            "batch=7000/2904, avg_acc=0.057158119658119656, avg_loss=6.820522280092592\n",
            "batch=7500/2904, avg_acc=0.05676529255319149, avg_loss=6.819942133477394\n",
            "batch=8000/2904, avg_acc=0.05656951371571072, avg_loss=6.812889163809227\n",
            "batch=8500/2904, avg_acc=0.05691754694835681, avg_loss=6.804689792400235\n",
            "batch=9000/2904, avg_acc=0.05713691796008869, avg_loss=6.795940011779379\n",
            "batch=9500/2904, avg_acc=0.05701811974789916, avg_loss=6.791005366990547\n",
            "batch=10000/2904, avg_acc=0.05712949101796407, avg_loss=6.784165263223553\n",
            "batch=10500/2904, avg_acc=0.05696292775665399, avg_loss=6.781524031606464\n",
            "batch=11000/2904, avg_acc=0.05735594373865699, avg_loss=6.773273735254083\n",
            "batch=11500/2904, avg_acc=0.05772569444444445, avg_loss=6.763521999782986\n",
            "batch=12000/2904, avg_acc=0.058163477537437606, avg_loss=6.751992772462563\n",
            "batch=12500/2904, avg_acc=0.05821186102236422, avg_loss=6.749375998402556\n",
            "batch=13000/2904, avg_acc=0.058501344086021506, avg_loss=6.741528057795699\n",
            "batch=13500/2904, avg_acc=0.058658468934911244, avg_loss=6.734680681397929\n",
            "batch=14000/2904, avg_acc=0.05893812410841655, avg_loss=6.72491752853067\n",
            "batch=14500/2904, avg_acc=0.05925878099173554, avg_loss=6.7164842673898075\n",
            "batch=15000/2904, avg_acc=0.05988681757656458, avg_loss=6.708612995173103\n",
            "batch=15500/2904, avg_acc=0.06015222293814433, avg_loss=6.701972756926547\n",
            "batch=16000/2904, avg_acc=0.06037375156054931, avg_loss=6.6943995786516854\n",
            "batch=16500/2904, avg_acc=0.06044188861985472, avg_loss=6.69142091026029\n",
            "batch=17000/2904, avg_acc=0.06038484136310223, avg_loss=6.68987312720329\n",
            "batch=17500/2904, avg_acc=0.060505850456621005, avg_loss=6.684473548087899\n",
            "batch=18000/2904, avg_acc=0.06057505549389567, avg_loss=6.682108854744728\n",
            "batch=18500/2904, avg_acc=0.06084300755939525, avg_loss=6.67424068574514\n",
            "batch=19000/2904, avg_acc=0.060942429022082016, avg_loss=6.669675834647739\n",
            "batch=19500/2904, avg_acc=0.061084784836065574, avg_loss=6.664234599129099\n",
            "batch=20000/2904, avg_acc=0.06118568931068931, avg_loss=6.6584204857642355\n",
            "batch=20500/2904, avg_acc=0.06141873781676413, avg_loss=6.655776376705653\n",
            "batch=21000/2904, avg_acc=0.06137012369172217, avg_loss=6.654257849666984\n",
            "batch=21500/2904, avg_acc=0.06126568308550186, avg_loss=6.650813923094796\n",
            "batch=22000/2904, avg_acc=0.06128519527702089, avg_loss=6.648626248864669\n",
            "batch=22500/2904, avg_acc=0.06149811278863233, avg_loss=6.64505023312611\n",
            "batch=23000/2904, avg_acc=0.06157417463075587, avg_loss=6.642726433536056\n",
            "batch=23500/2904, avg_acc=0.061593856292517006, avg_loss=6.640478847789115\n",
            "batch=24000/2904, avg_acc=0.061604912572855955, avg_loss=6.639060548501249\n",
            "batch=24500/2904, avg_acc=0.06177865008156607, avg_loss=6.634944688009788\n",
            "batch=25000/2904, avg_acc=0.0617630895283773, avg_loss=6.633620603517186\n",
            "batch=25500/2904, avg_acc=0.061730995297805645, avg_loss=6.632946586010972\n",
            "batch=26000/2904, avg_acc=0.06194513835511145, avg_loss=6.6297871829362025\n",
            "batch=26500/2904, avg_acc=0.06196973981900453, avg_loss=6.628423713235295\n",
            "batch=27000/2904, avg_acc=0.06194254256106588, avg_loss=6.626852794226499\n",
            "batch=27500/2904, avg_acc=0.06200263444767442, avg_loss=6.624605968386628\n",
            "batch=28000/2904, avg_acc=0.06196912919343326, avg_loss=6.623085073162027\n",
            "batch=28500/2904, avg_acc=0.06212307152875175, avg_loss=6.619390997545582\n",
            "batch=29000/2904, avg_acc=0.06215110268780152, avg_loss=6.618046282736044\n",
            "batch=29500/2904, avg_acc=0.06221841124661247, avg_loss=6.617081639566396\n",
            "batch=30000/2904, avg_acc=0.06230013324450366, avg_loss=6.614145257328448\n",
            "batch=30500/2904, avg_acc=0.062299311926605504, avg_loss=6.6130227105176935\n",
            "batch=31000/2904, avg_acc=0.06232269503546099, avg_loss=6.610249939555126\n",
            "batch=31500/2904, avg_acc=0.062402839467005075, avg_loss=6.607439720812183\n",
            "batch=32000/2904, avg_acc=0.062490240474703314, avg_loss=6.605107647564022\n",
            "batch=32500/2904, avg_acc=0.06253075030750307, avg_loss=6.603843788437884\n",
            "batch=33000/2904, avg_acc=0.06259463961235615, avg_loss=6.602147372804361\n",
            "batch=33500/2904, avg_acc=0.0625522076372315, avg_loss=6.601360661545346\n",
            "batch=34000/2904, avg_acc=0.06281231628453851, avg_loss=6.598327730011758\n",
            "batch=34500/2904, avg_acc=0.06280960312862109, avg_loss=6.596316899623407\n",
            "batch=35000/2904, avg_acc=0.06289441747572816, avg_loss=6.593832095945174\n",
            "batch=35500/2904, avg_acc=0.06294869087837837, avg_loss=6.592830623592342\n",
            "batch=36000/2904, avg_acc=0.06294940310938367, avg_loss=6.592218298861743\n",
            "batch=36500/2904, avg_acc=0.06301341730558598, avg_loss=6.590092329545454\n",
            "batch=37000/2904, avg_acc=0.06299466504592112, avg_loss=6.588424753511616\n",
            "batch=37500/2904, avg_acc=0.06298640724946696, avg_loss=6.58627065565032\n",
            "batch=38000/2904, avg_acc=0.06305727248816412, avg_loss=6.584845558258811\n",
            "batch=38500/2904, avg_acc=0.06308735721703011, avg_loss=6.584104036863967\n",
            "batch=39000/2904, avg_acc=0.06307662737057919, avg_loss=6.583357893388007\n",
            "batch=39500/2904, avg_acc=0.0630867282388664, avg_loss=6.582198886639676\n",
            "batch=40000/2904, avg_acc=0.06308564467766117, avg_loss=6.581709926286856\n",
            "batch=40500/2904, avg_acc=0.06317559230009871, avg_loss=6.5792548432872655\n",
            "batch=41000/2904, avg_acc=0.06315669185763043, avg_loss=6.579190029254023\n",
            "batch=41500/2904, avg_acc=0.0631231936416185, avg_loss=6.578704539980732\n",
            "batch=42000/2904, avg_acc=0.06314701332698715, avg_loss=6.5779509757258445\n",
            "batch=42500/2904, avg_acc=0.06321142991533396, avg_loss=6.575817262464723\n",
            "batch=43000/2904, avg_acc=0.0632365760111576, avg_loss=6.575260053463506\n",
            "batch=43500/2904, avg_acc=0.06329130284926471, avg_loss=6.573893468520221\n",
            "batch=44000/2904, avg_acc=0.06335188550658792, avg_loss=6.572639567242162\n",
            "batch=44500/2904, avg_acc=0.06330020215633424, avg_loss=6.57218665768194\n",
            "batch=45000/2904, avg_acc=0.063249666814749, avg_loss=6.571557779875611\n",
            "batch=45500/2904, avg_acc=0.06321534490333919, avg_loss=6.572075461335676\n",
            "batch=46000/2904, avg_acc=0.06323201868752716, avg_loss=6.571485223815732\n",
            "batch=46500/2904, avg_acc=0.06318921969045571, avg_loss=6.570696071582115\n",
            "batch=47000/2904, avg_acc=0.06324037643555934, avg_loss=6.569577440450872\n",
            "batch=47500/2904, avg_acc=0.06327072811447812, avg_loss=6.568818392255892\n",
            "batch=48000/2904, avg_acc=0.06325229071220324, avg_loss=6.567947599958351\n",
            "batch=48500/2904, avg_acc=0.06331280915086562, avg_loss=6.566377267106348\n",
            "batch=49000/2904, avg_acc=0.06333639330885353, avg_loss=6.564905268257854\n",
            "batch=49500/2904, avg_acc=0.06351978998384492, avg_loss=6.561844961631664\n",
            "batch=50000/2904, avg_acc=0.06370201919232307, avg_loss=6.559438099760096\n",
            "batch=50500/2904, avg_acc=0.06380393903404592, avg_loss=6.558136629057799\n",
            "batch=51000/2904, avg_acc=0.06378381027048216, avg_loss=6.557678361426891\n",
            "batch=51500/2904, avg_acc=0.06375679347826087, avg_loss=6.557267202057454\n",
            "batch=52000/2904, avg_acc=0.06378075740099962, avg_loss=6.557442450019224\n",
            "batch=52500/2904, avg_acc=0.06381735529322163, avg_loss=6.556620097105864\n",
            "batch=53000/2904, avg_acc=0.06381907770652584, avg_loss=6.555687122783855\n",
            "batch=53500/2904, avg_acc=0.06390251307922272, avg_loss=6.55370889387145\n",
            "batch=54000/2904, avg_acc=0.0638640781192151, avg_loss=6.553340198074787\n",
            "batch=54500/2904, avg_acc=0.06391920396184886, avg_loss=6.552452081804843\n",
            "batch=55000/2904, avg_acc=0.06387450018175209, avg_loss=6.55202596782988\n",
            "batch=55500/2904, avg_acc=0.06384636167146975, avg_loss=6.5516345461095105\n",
            "batch=56000/2904, avg_acc=0.0638488486254909, avg_loss=6.550752521420921\n",
            "batch=56500/2904, avg_acc=0.06386124380750177, avg_loss=6.549984518754423\n",
            "batch=57000/2904, avg_acc=0.06392822693791653, avg_loss=6.54911270168362\n",
            "batch=57500/2904, avg_acc=0.06398752607788595, avg_loss=6.548327212273992\n",
            "batch=58000/2904, avg_acc=0.06394454498448811, avg_loss=6.547640899689762\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.06394365651732388), array([6.547626], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_ptb(model_rnn, train_data, seq_len=20, n_epochs=1, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(num_samples):\n",
        "  y = model_rnn.generate(x, max_new_tokens, temperature=temperature, corpus=corpus)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    idxs = y[:,i].astype(\"int32\")\n",
        "    print(corpus.decode(idxs))\n",
        "\n",
        "  print('---------------')"
      ],
      "metadata": {
        "id": "Hd6HDE2ZSXna",
        "outputId": "fc22debb-0e51-466b-91fd-0adfed970f34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Hd6HDE2ZSXna",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> <unk> am a pennsylvania tree u.k. said futures he for the innocent <eos> the treasury $ N million deck the same <eos> and <unk> given to benchmark because each the <unk> in the china and with selling more the the new than 's and an and in only the where to keep the end in foreign of 's the government the unchanged to sell the commodity and a in however york in N in the paid firms high in <unk> in N for of only <unk> <unk> about high in what to london is sides on the other local is the financial and in to <unk> <unk> am a pennsylvania tree u.k. said futures he for the innocent <eos> the treasury $ N million deck the same <eos> and <unk> given to benchmark because each the <unk> in the china and with selling more the the new than 's and an and in only the where to keep the end in foreign of 's the government the unchanged to sell the commodity and a in however york in N in the paid firms high in <unk> in N for of only <unk> <unk> about high in what to london is sides on the other local is the financial and in to\n",
            "---------------\n",
            "<unk> <unk> am a pennsylvania tree the market was home cable in N under are two is <unk> in N million or and involved the pa to help other containing industrials <unk> <eos> up <unk> from that on to the corn most <eos> the big company and $ N cents by new the only they theft <unk> N billion would <unk> out securities boston a to make their a as administration plenty advantage maker N <unk> said state called today debate the new a the finding the made <unk> in something to $ N his primarily still a left the only made a do trillion the <unk> <unk> am a pennsylvania tree the market was home cable in N under are two is <unk> in N million or and involved the pa to help other containing industrials <unk> <eos> up <unk> from that on to the corn most <eos> the big company and $ N cents by new the only they theft <unk> N billion would <unk> out securities boston a to make their a as administration plenty advantage maker N <unk> said state called today debate the new a the finding the made <unk> in something to $ N his primarily still a left the only made a do trillion the\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ptb(model_rnn, train_data, seq_len=20, n_epochs=2, device=device)"
      ],
      "metadata": {
        "id": "oOQHF0IKY0pX",
        "outputId": "f1384e3d-8426-485f-d47f-a08d6f273625",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oOQHF0IKY0pX",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch i=0\n",
            "batch=0/2904, avg_acc=0.078125, avg_loss=6.823260498046875\n",
            "batch=500/2904, avg_acc=0.059495192307692304, avg_loss=6.5485698993389425\n",
            "batch=1000/2904, avg_acc=0.0610906862745098, avg_loss=6.534269684436275\n",
            "batch=1500/2904, avg_acc=0.0623766447368421, avg_loss=6.523627030222039\n",
            "batch=2000/2904, avg_acc=0.06305693069306931, avg_loss=6.508475788985148\n",
            "batch=2500/2904, avg_acc=0.06428571428571428, avg_loss=6.492670355902778\n",
            "batch=3000/2904, avg_acc=0.06469370860927152, avg_loss=6.485643884519868\n",
            "batch=3500/2904, avg_acc=0.06576704545454545, avg_loss=6.4721912730823865\n",
            "batch=4000/2904, avg_acc=0.06634017412935324, avg_loss=6.469325734608209\n",
            "batch=4500/2904, avg_acc=0.06653761061946903, avg_loss=6.463664529175885\n",
            "batch=5000/2904, avg_acc=0.0670941235059761, avg_loss=6.455280051668327\n",
            "batch=5500/2904, avg_acc=0.06769701086956521, avg_loss=6.442752207880435\n",
            "batch=6000/2904, avg_acc=0.06782599667774086, avg_loss=6.439454422757475\n",
            "batch=6500/2904, avg_acc=0.06713957055214724, avg_loss=6.449814872507669\n",
            "batch=7000/2904, avg_acc=0.0668625356125356, avg_loss=6.451274261039886\n",
            "batch=7500/2904, avg_acc=0.06627327127659574, avg_loss=6.459215009973405\n",
            "batch=8000/2904, avg_acc=0.06601465087281795, avg_loss=6.4626680369389025\n",
            "batch=8500/2904, avg_acc=0.06606514084507042, avg_loss=6.463465100498826\n",
            "batch=9000/2904, avg_acc=0.06625554323725055, avg_loss=6.461278755543237\n",
            "batch=9500/2904, avg_acc=0.0663405987394958, avg_loss=6.46346548384979\n",
            "batch=10000/2904, avg_acc=0.06647330339321357, avg_loss=6.463271503867266\n",
            "batch=10500/2904, avg_acc=0.06618346007604563, avg_loss=6.467101354562738\n",
            "batch=11000/2904, avg_acc=0.06652109800362976, avg_loss=6.46392992853902\n",
            "batch=11500/2904, avg_acc=0.06678602430555555, avg_loss=6.458675808376736\n",
            "batch=12000/2904, avg_acc=0.06716930116472546, avg_loss=6.45218256031614\n",
            "batch=12500/2904, avg_acc=0.06695786741214058, avg_loss=6.455505566094249\n",
            "batch=13000/2904, avg_acc=0.06696908602150538, avg_loss=6.453181403609831\n",
            "batch=13500/2904, avg_acc=0.06703032544378698, avg_loss=6.45147582285503\n",
            "batch=14000/2904, avg_acc=0.06731455064194009, avg_loss=6.445696995363766\n",
            "batch=14500/2904, avg_acc=0.06760933195592286, avg_loss=6.44141647296832\n",
            "batch=15000/2904, avg_acc=0.06799683754993342, avg_loss=6.437348639314248\n",
            "batch=15500/2904, avg_acc=0.06810164304123711, avg_loss=6.435080742590206\n",
            "batch=16000/2904, avg_acc=0.06813748439450687, avg_loss=6.431558696161049\n",
            "batch=16500/2904, avg_acc=0.06803874092009685, avg_loss=6.432425658292979\n",
            "batch=17000/2904, avg_acc=0.0680008813160987, avg_loss=6.434435131462985\n",
            "batch=17500/2904, avg_acc=0.06810074200913242, avg_loss=6.432953410388128\n",
            "batch=18000/2904, avg_acc=0.06802857935627081, avg_loss=6.434006919395117\n",
            "batch=18500/2904, avg_acc=0.06825728941684665, avg_loss=6.429209132019438\n",
            "batch=19000/2904, avg_acc=0.06836882229232387, avg_loss=6.427708497634069\n",
            "batch=19500/2904, avg_acc=0.06846823770491803, avg_loss=6.424247166367828\n",
            "batch=20000/2904, avg_acc=0.06864385614385614, avg_loss=6.420755416458541\n",
            "batch=20500/2904, avg_acc=0.06880787037037037, avg_loss=6.420724445662768\n",
            "batch=21000/2904, avg_acc=0.06868458610846813, avg_loss=6.422286066841104\n",
            "batch=21500/2904, avg_acc=0.06865706319702602, avg_loss=6.421416124535316\n",
            "batch=22000/2904, avg_acc=0.06862511353315168, avg_loss=6.422006982288829\n",
            "batch=22500/2904, avg_acc=0.06876387655417407, avg_loss=6.421157582149201\n",
            "batch=23000/2904, avg_acc=0.06887489139878367, avg_loss=6.421351677888793\n",
            "batch=23500/2904, avg_acc=0.06890678146258504, avg_loss=6.421302349064626\n",
            "batch=24000/2904, avg_acc=0.06883326394671108, avg_loss=6.422291970233139\n",
            "batch=24500/2904, avg_acc=0.06907371533442089, avg_loss=6.420387693719412\n",
            "batch=25000/2904, avg_acc=0.06901229016786571, avg_loss=6.421722621902478\n",
            "batch=25500/2904, avg_acc=0.06895082288401254, avg_loss=6.423330353644201\n",
            "batch=26000/2904, avg_acc=0.06903823981552652, avg_loss=6.422437067640277\n",
            "batch=26500/2904, avg_acc=0.06900452488687783, avg_loss=6.423238947021116\n",
            "batch=27000/2904, avg_acc=0.0689049777942265, avg_loss=6.423713337342709\n",
            "batch=27500/2904, avg_acc=0.06897710755813953, avg_loss=6.423476108284884\n",
            "batch=28000/2904, avg_acc=0.06888160242683797, avg_loss=6.423999040863669\n",
            "batch=28500/2904, avg_acc=0.06901078190743339, avg_loss=6.422250832748948\n",
            "batch=29000/2904, avg_acc=0.06895460027567195, avg_loss=6.422737551688491\n",
            "batch=29500/2904, avg_acc=0.06892149390243903, avg_loss=6.423760903624661\n",
            "batch=30000/2904, avg_acc=0.06904980013324451, avg_loss=6.422564123917389\n",
            "batch=30500/2904, avg_acc=0.06904693643512451, avg_loss=6.422939875491481\n",
            "batch=31000/2904, avg_acc=0.06908043197936815, avg_loss=6.421725902643455\n",
            "batch=31500/2904, avg_acc=0.06916640228426396, avg_loss=6.420504342480965\n",
            "batch=32000/2904, avg_acc=0.06927115865084323, avg_loss=6.419906015771393\n",
            "batch=32500/2904, avg_acc=0.06923623923739237, avg_loss=6.4204090751845015\n",
            "batch=33000/2904, avg_acc=0.06925348273773471, avg_loss=6.420415183979406\n",
            "batch=33500/2904, avg_acc=0.06919562947494033, avg_loss=6.4212862656622915\n",
            "batch=34000/2904, avg_acc=0.06949404761904762, avg_loss=6.419582231040565\n",
            "batch=34500/2904, avg_acc=0.06948327056778679, avg_loss=6.419025202780997\n",
            "batch=35000/2904, avg_acc=0.0695816676185037, avg_loss=6.417933948458024\n",
            "batch=35500/2904, avg_acc=0.06959459459459459, avg_loss=6.41827051661036\n",
            "batch=36000/2904, avg_acc=0.06956378400888395, avg_loss=6.419068833287063\n",
            "batch=36500/2904, avg_acc=0.06960569550930996, avg_loss=6.4183302813526835\n",
            "batch=37000/2904, avg_acc=0.06955868449486764, avg_loss=6.4179746589681255\n",
            "batch=37500/2904, avg_acc=0.06956456556503199, avg_loss=6.41714002531983\n",
            "batch=38000/2904, avg_acc=0.06962782745923199, avg_loss=6.416991468306155\n",
            "batch=38500/2904, avg_acc=0.0696553738317757, avg_loss=6.417428446261682\n",
            "batch=39000/2904, avg_acc=0.06963576371091748, avg_loss=6.417817385315223\n",
            "batch=39500/2904, avg_acc=0.06964669787449393, avg_loss=6.417747738486842\n",
            "batch=40000/2904, avg_acc=0.06961206896551723, avg_loss=6.418547757371314\n",
            "batch=40500/2904, avg_acc=0.0696970631786772, avg_loss=6.417352541954591\n",
            "batch=41000/2904, avg_acc=0.06964590443686007, avg_loss=6.418438414188201\n",
            "batch=41500/2904, avg_acc=0.0695688824662813, avg_loss=6.419184278660886\n",
            "batch=42000/2904, avg_acc=0.06958739885768682, avg_loss=6.419555420038077\n",
            "batch=42500/2904, avg_acc=0.06965104656632173, avg_loss=6.418375911335842\n",
            "batch=43000/2904, avg_acc=0.06962168758716876, avg_loss=6.418831357508136\n",
            "batch=43500/2904, avg_acc=0.06961598115808823, avg_loss=6.418574075137868\n",
            "batch=44000/2904, avg_acc=0.06964164016356202, avg_loss=6.418352453430259\n",
            "batch=44500/2904, avg_acc=0.06959793351302786, avg_loss=6.418830020215633\n",
            "batch=45000/2904, avg_acc=0.06953576188360729, avg_loss=6.419270601954687\n",
            "batch=45500/2904, avg_acc=0.06946809094903339, avg_loss=6.420736764059754\n",
            "batch=46000/2904, avg_acc=0.06944534984789222, avg_loss=6.421222430465015\n",
            "batch=46500/2904, avg_acc=0.06941638005159072, avg_loss=6.4213926805674975\n",
            "batch=47000/2904, avg_acc=0.06943587834963845, avg_loss=6.421432369204593\n",
            "batch=47500/2904, avg_acc=0.06948916245791245, avg_loss=6.421540272516835\n",
            "batch=48000/2904, avg_acc=0.06946454602249062, avg_loss=6.42162445335277\n",
            "batch=48500/2904, avg_acc=0.06949582646331409, avg_loss=6.420997784418796\n",
            "batch=49000/2904, avg_acc=0.06947674418604652, avg_loss=6.420534985720114\n",
            "batch=49500/2904, avg_acc=0.06968270395799676, avg_loss=6.418284910137318\n",
            "batch=50000/2904, avg_acc=0.06983581567373051, avg_loss=6.416772041183527\n",
            "batch=50500/2904, avg_acc=0.06991909144893112, avg_loss=6.4163227434679335\n",
            "batch=51000/2904, avg_acc=0.06988803410427283, avg_loss=6.416581732653861\n",
            "batch=51500/2904, avg_acc=0.06987698951863354, avg_loss=6.416851465450311\n",
            "batch=52000/2904, avg_acc=0.06987576893502499, avg_loss=6.4178200692041525\n",
            "batch=52500/2904, avg_acc=0.06989004188880427, avg_loss=6.417872953160701\n",
            "batch=53000/2904, avg_acc=0.06985453602414184, avg_loss=6.417660198981516\n",
            "batch=53500/2904, avg_acc=0.06991778774289985, avg_loss=6.416468726644245\n",
            "batch=54000/2904, avg_acc=0.06984103109959275, avg_loss=6.4169456914105885\n",
            "batch=54500/2904, avg_acc=0.06984822083639032, avg_loss=6.416908359317682\n",
            "batch=55000/2904, avg_acc=0.06980870592511813, avg_loss=6.417293711377681\n",
            "batch=55500/2904, avg_acc=0.06975301693083573, avg_loss=6.417783006123919\n",
            "batch=56000/2904, avg_acc=0.0697541056765441, avg_loss=6.417768765619422\n",
            "batch=56500/2904, avg_acc=0.0697463287331918, avg_loss=6.417833289101203\n",
            "batch=57000/2904, avg_acc=0.0697737635917222, avg_loss=6.417776657313223\n",
            "batch=57500/2904, avg_acc=0.06979963490959666, avg_loss=6.417724269819193\n",
            "batch=58000/2904, avg_acc=0.06971841606342641, avg_loss=6.417831135815236\n",
            "epoch i=1\n",
            "batch=0/2904, avg_acc=0.084375, avg_loss=6.831170654296875\n",
            "batch=500/2904, avg_acc=0.0609375, avg_loss=6.520606407752404\n",
            "batch=1000/2904, avg_acc=0.06390931372549019, avg_loss=6.503447169883579\n",
            "batch=1500/2904, avg_acc=0.06492598684210527, avg_loss=6.487618857935855\n",
            "batch=2000/2904, avg_acc=0.06556311881188119, avg_loss=6.472223081683168\n",
            "batch=2500/2904, avg_acc=0.06688988095238095, avg_loss=6.455488901289683\n",
            "batch=3000/2904, avg_acc=0.06767384105960265, avg_loss=6.448579780629139\n",
            "batch=3500/2904, avg_acc=0.06949573863636363, avg_loss=6.433749112215909\n",
            "batch=4000/2904, avg_acc=0.06997823383084577, avg_loss=6.430022737873134\n",
            "batch=4500/2904, avg_acc=0.07018805309734513, avg_loss=6.42474635301438\n",
            "batch=5000/2904, avg_acc=0.07076693227091634, avg_loss=6.4157802384213145\n",
            "batch=5500/2904, avg_acc=0.07109375, avg_loss=6.404577813632247\n",
            "batch=6000/2904, avg_acc=0.07125207641196013, avg_loss=6.401365240863788\n",
            "batch=6500/2904, avg_acc=0.07037001533742332, avg_loss=6.4118062212423315\n",
            "batch=7000/2904, avg_acc=0.07018340455840456, avg_loss=6.413651286502849\n",
            "batch=7500/2904, avg_acc=0.06974734042553192, avg_loss=6.4206859832114365\n",
            "batch=8000/2904, avg_acc=0.06935006234413965, avg_loss=6.424762800031172\n",
            "batch=8500/2904, avg_acc=0.06934419014084507, avg_loss=6.425826639524648\n",
            "batch=9000/2904, avg_acc=0.06947062084257206, avg_loss=6.424255127494456\n",
            "batch=9500/2904, avg_acc=0.06929490546218488, avg_loss=6.426910451680672\n",
            "batch=10000/2904, avg_acc=0.06934256487025948, avg_loss=6.4273609031936125\n",
            "batch=10500/2904, avg_acc=0.06904111216730038, avg_loss=6.430754663735741\n",
            "batch=11000/2904, avg_acc=0.06903924682395644, avg_loss=6.428210781533576\n",
            "batch=11500/2904, avg_acc=0.06936848958333333, avg_loss=6.423496500651042\n",
            "batch=12000/2904, avg_acc=0.0697119384359401, avg_loss=6.417671069051581\n",
            "batch=12500/2904, avg_acc=0.06938897763578275, avg_loss=6.421209190295527\n",
            "batch=13000/2904, avg_acc=0.06937403993855606, avg_loss=6.419233030913978\n",
            "batch=13500/2904, avg_acc=0.0693648298816568, avg_loss=6.417795395710059\n",
            "batch=14000/2904, avg_acc=0.06964604136947218, avg_loss=6.41233059914408\n",
            "batch=14500/2904, avg_acc=0.06984331955922865, avg_loss=6.408498514979339\n",
            "batch=15000/2904, avg_acc=0.07015229693741677, avg_loss=6.4046677346870835\n",
            "batch=15500/2904, avg_acc=0.07026820231958762, avg_loss=6.4025586944265465\n",
            "batch=16000/2904, avg_acc=0.0704002808988764, avg_loss=6.399156328027465\n",
            "batch=16500/2904, avg_acc=0.07026331719128329, avg_loss=6.400213756053269\n",
            "batch=17000/2904, avg_acc=0.07015643360752057, avg_loss=6.402609062867215\n",
            "batch=17500/2904, avg_acc=0.07026969178082192, avg_loss=6.401245897545662\n",
            "batch=18000/2904, avg_acc=0.07018243618201998, avg_loss=6.402553152746948\n",
            "batch=18500/2904, avg_acc=0.07035974622030237, avg_loss=6.397973896463283\n",
            "batch=19000/2904, avg_acc=0.07043243953732913, avg_loss=6.39655748882755\n",
            "batch=19500/2904, avg_acc=0.07052702356557378, avg_loss=6.3933773853739755\n",
            "batch=20000/2904, avg_acc=0.07065434565434565, avg_loss=6.389980722402598\n",
            "batch=20500/2904, avg_acc=0.07083028752436647, avg_loss=6.390186403508772\n",
            "batch=21000/2904, avg_acc=0.07070944338725024, avg_loss=6.3918648905804\n",
            "batch=21500/2904, avg_acc=0.07064939591078066, avg_loss=6.391144139172862\n",
            "batch=22000/2904, avg_acc=0.07060059037238874, avg_loss=6.391848319709355\n",
            "batch=22500/2904, avg_acc=0.07072047069271759, avg_loss=6.391250138765542\n",
            "batch=23000/2904, avg_acc=0.07078898783666376, avg_loss=6.39162752497828\n",
            "batch=23500/2904, avg_acc=0.07078815901360544, avg_loss=6.391667330994898\n",
            "batch=24000/2904, avg_acc=0.0707509367194005, avg_loss=6.392766444629475\n",
            "batch=24500/2904, avg_acc=0.07098796900489396, avg_loss=6.391081897430669\n",
            "batch=25000/2904, avg_acc=0.0709957034372502, avg_loss=6.392177507993605\n",
            "batch=25500/2904, avg_acc=0.07091986677115987, avg_loss=6.393948373824451\n",
            "batch=26000/2904, avg_acc=0.071005476556495, avg_loss=6.393244379323598\n",
            "batch=26500/2904, avg_acc=0.07089932126696832, avg_loss=6.3945389093137255\n",
            "batch=27000/2904, avg_acc=0.07075777202072539, avg_loss=6.395147113249445\n",
            "batch=27500/2904, avg_acc=0.07082803415697675, avg_loss=6.3949763808139535\n",
            "batch=28000/2904, avg_acc=0.07072403640256959, avg_loss=6.39566102337616\n",
            "batch=28500/2904, avg_acc=0.070851595371669, avg_loss=6.39413898141655\n",
            "batch=29000/2904, avg_acc=0.07079815644383183, avg_loss=6.394769770847692\n",
            "batch=29500/2904, avg_acc=0.07077193428184282, avg_loss=6.395871443089431\n",
            "batch=30000/2904, avg_acc=0.0708673384410393, avg_loss=6.394907561625583\n",
            "batch=30500/2904, avg_acc=0.07084084207077326, avg_loss=6.39553366644823\n",
            "batch=31000/2904, avg_acc=0.07086355577047067, avg_loss=6.394448642005158\n",
            "batch=31500/2904, avg_acc=0.07096089784263959, avg_loss=6.393386143718274\n",
            "batch=32000/2904, avg_acc=0.07104544034978139, avg_loss=6.392858467364148\n",
            "batch=32500/2904, avg_acc=0.07101014760147602, avg_loss=6.393408383302583\n",
            "batch=33000/2904, avg_acc=0.07099674439733494, avg_loss=6.393559774379164\n",
            "batch=33500/2904, avg_acc=0.07092593973747016, avg_loss=6.394541038931981\n",
            "batch=34000/2904, avg_acc=0.07121546149323928, avg_loss=6.392932925485009\n",
            "batch=34500/2904, avg_acc=0.07123768829663962, avg_loss=6.392459534327926\n",
            "batch=35000/2904, avg_acc=0.07133066818960594, avg_loss=6.391397326527699\n",
            "batch=35500/2904, avg_acc=0.07133833051801802, avg_loss=6.391846582911036\n",
            "batch=36000/2904, avg_acc=0.07133189894503054, avg_loss=6.392668569544697\n",
            "batch=36500/2904, avg_acc=0.0713684282584885, avg_loss=6.392042031763418\n",
            "batch=37000/2904, avg_acc=0.0713212452728255, avg_loss=6.391791177066451\n",
            "batch=37500/2904, avg_acc=0.07132695895522388, avg_loss=6.39105435434435\n",
            "batch=38000/2904, avg_acc=0.07139170173592846, avg_loss=6.390996925960021\n",
            "batch=38500/2904, avg_acc=0.07139148494288682, avg_loss=6.391516987928349\n",
            "batch=39000/2904, avg_acc=0.07135283188108663, avg_loss=6.391961253844182\n",
            "batch=39500/2904, avg_acc=0.07135627530364372, avg_loss=6.391954232034413\n",
            "batch=40000/2904, avg_acc=0.07131121939030485, avg_loss=6.392811797226387\n",
            "batch=40500/2904, avg_acc=0.07139529861796644, avg_loss=6.391640702122409\n",
            "batch=41000/2904, avg_acc=0.07133105802047782, avg_loss=6.392784769624574\n",
            "batch=41500/2904, avg_acc=0.0712262162813102, avg_loss=6.393672477119461\n",
            "batch=42000/2904, avg_acc=0.07123542360780581, avg_loss=6.394116640885293\n",
            "batch=42500/2904, avg_acc=0.07130027046095955, avg_loss=6.393016521636877\n",
            "batch=43000/2904, avg_acc=0.07126191306369131, avg_loss=6.3935226348210135\n",
            "batch=43500/2904, avg_acc=0.07125315946691177, avg_loss=6.393366555606618\n",
            "batch=44000/2904, avg_acc=0.07126306224443435, avg_loss=6.393214731940027\n",
            "batch=44500/2904, avg_acc=0.07119833782569632, avg_loss=6.39381457771788\n",
            "batch=45000/2904, avg_acc=0.0711058973789427, avg_loss=6.394323356286095\n",
            "batch=45500/2904, avg_acc=0.0710319639718805, avg_loss=6.3958960347100176\n",
            "batch=46000/2904, avg_acc=0.07100581269013473, avg_loss=6.396477075184702\n",
            "batch=46500/2904, avg_acc=0.07098425408426483, avg_loss=6.396735946904557\n",
            "batch=47000/2904, avg_acc=0.07097777541471714, avg_loss=6.396678939812846\n",
            "batch=47500/2904, avg_acc=0.0710016835016835, avg_loss=6.396817787247475\n",
            "batch=48000/2904, avg_acc=0.0709899521032903, avg_loss=6.396988234069138\n",
            "batch=48500/2904, avg_acc=0.07104673330585326, avg_loss=6.3964177143446\n",
            "batch=49000/2904, avg_acc=0.07102075683394533, avg_loss=6.396027131782946\n",
            "batch=49500/2904, avg_acc=0.07121869951534733, avg_loss=6.393888201736672\n",
            "batch=50000/2904, avg_acc=0.07135395841663335, avg_loss=6.392406787285086\n",
            "batch=50500/2904, avg_acc=0.07142963182897863, avg_loss=6.391952444576406\n",
            "batch=51000/2904, avg_acc=0.07140337122696981, avg_loss=6.3922132252058015\n",
            "batch=51500/2904, avg_acc=0.07143706327639751, avg_loss=6.392375533773292\n",
            "batch=52000/2904, avg_acc=0.07143045943867743, avg_loss=6.393536740676663\n",
            "batch=52500/2904, avg_acc=0.07143350152322925, avg_loss=6.393641707920792\n",
            "batch=53000/2904, avg_acc=0.07139287061486231, avg_loss=6.393465909090909\n",
            "batch=53500/2904, avg_acc=0.07145109304932735, avg_loss=6.39234865470852\n",
            "batch=54000/2904, avg_acc=0.07136130136986302, avg_loss=6.39290366993706\n",
            "batch=54500/2904, avg_acc=0.07135454878943508, avg_loss=6.3930335198092445\n",
            "batch=55000/2904, avg_acc=0.07129339331152308, avg_loss=6.393484755543438\n",
            "batch=55500/2904, avg_acc=0.07123896793948127, avg_loss=6.394002724243516\n",
            "batch=56000/2904, avg_acc=0.07122567832916815, avg_loss=6.393941895751517\n",
            "batch=56500/2904, avg_acc=0.07121704706298655, avg_loss=6.394050778485492\n",
            "batch=57000/2904, avg_acc=0.07124364258155033, avg_loss=6.394052525429673\n",
            "batch=57500/2904, avg_acc=0.07127412204450626, avg_loss=6.394070540681502\n",
            "batch=58000/2904, avg_acc=0.07119850913478111, avg_loss=6.394209970699759\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.07119636310435636), array([6.3943086], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training on 3 epochs\n",
        "for k in range(num_samples):\n",
        "  y = model_rnn.generate(x, max_new_tokens, temperature=temperature, corpus=corpus)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    idxs = y[:,i].astype(\"int32\")\n",
        "    # print(corpus.decode(idxs))\n",
        "\n",
        "  print('---------------')"
      ],
      "metadata": {
        "id": "OPVFp1NgcSVz",
        "outputId": "d9c4a2a5-bd51-4de9-d2eb-c930b6de5857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OPVFp1NgcSVz",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> <unk> am a pennsylvania tree expanded that legislators those national the weekend <eos> the first eliminate the whole truck the world <eos> of <unk> points used test $ N billion <unk> in the mulford and government person has the the name has in and state of in they the money to meet the country in she is in the house the drive british say to focus the u.s. in exceed york in N in the action priority to $ N million the big president total <unk> <unk> in a in world to sell to recruit in the president earlier a the government of in to ---------------\n",
            "<unk> <unk> am a pennsylvania tree the short-term and nbc causes in N for are could a <unk> in the rest or and propose the guarantees to hand by pension futures <unk> <eos> that <unk> in to keep a the closing president <eos> will raise the market $ N million which that the market because acknowledged <unk> <unk> on ministry N people $ N billion to make government as as gas prices richmond been <unk> <unk> with noted many in services the new to the u.s. the money <unk> to possibility a in in deputy potential out a certain the market n't as administration shares the ---------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}